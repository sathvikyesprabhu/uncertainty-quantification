{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms, datasets\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from scipy.special import softmax\n",
    "\n",
    "from networks import ResNet\n",
    "from utils import *\n",
    "from utils_plotting import *\n",
    "import tent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "mean = (0.4914, 0.4822, 0.4465)\n",
    "std = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "transform_train = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.RandomCrop(32, padding=4),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean, std)\n",
    "                                    ])\n",
    "transform_test = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean, std)])\n",
    "\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "valset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "print(trainset.train_data.shape)\n",
    "\n",
    "models_dir = \"Models/\" + 'Tent_models'\n",
    "results_dir = \"Results/\" + 'Tent_results'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet(18,10).to(device)\n",
    "print(sum(p.numel() for p in model.parameters())/1000000, \"M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(epoch): \n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    lr = 0.1\n",
    "    \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate(lr, epoch), momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "    print('\\n=> Training Epoch #%d, LR=%.4f' %(epoch, learning_rate(lr, epoch)))\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda() # GPU settings\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = Variable(inputs), Variable(targets)\n",
    "        outputs = model(inputs)               # Forward Propagation\n",
    "        loss = criterion(outputs, targets)  # Loss\n",
    "        loss.backward()  # Backward Propagation\n",
    "        optimizer.step() # Optimizer update\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "        # sys.stdout.write('\\r')\n",
    "        # sys.stdout.write('| Epoch [%3d/%3d] Iter[%3d/%3d]\\t\\tLoss: %.4f Acc@1: %.3f%%'\n",
    "        #         %(epoch, num_epochs, batch_idx+1,\n",
    "        #             (len(trainset)//batch_size)+1, loss.item(), 100.*correct/total))\n",
    "        # sys.stdout.flush()\n",
    "    print('| Epoch [%3d/%3d] Iter[%3d/%3d]\\t\\tLoss: %.4f Acc@1: %.3f%%'\n",
    "                %(epoch, num_epochs, batch_idx+1,\n",
    "                    (len(trainset)//batch_size)+1, loss.item(), 100.*correct/total))\n",
    "\n",
    "def test(epoch):\n",
    "    global best_acc, model_best\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(valloader):\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        with torch.no_grad():\n",
    "            inputs, targets = Variable(inputs), Variable(targets)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "    # Save checkpoint when best model\n",
    "    acc = 100.*correct/total\n",
    "    print(\"\\n| Validation Epoch #%d\\t\\t\\tLoss: %.4f Acc@1: %.2f%%\" %(epoch, loss.item(), acc))\n",
    "\n",
    "    if acc > best_acc:\n",
    "        model_best = model\n",
    "        # print(acc, best_acc)\n",
    "        best_acc = acc\n",
    "        print('| Saving Best model...\\t\\t\\tTop1 = %.2f%%' %(acc))\n",
    "\n",
    "        # state = {\n",
    "        #         'net':net.module if use_cuda else net,\n",
    "        #         'acc':acc,\n",
    "        #         'epoch':epoch,\n",
    "        # }\n",
    "\n",
    "        # torch.save(state, models_dir + '/' + 'theta_best.t7')\n",
    "        torch.save(model.state_dict(),models_dir + '/' + 'theta_best.pt')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "best_acc = 50\n",
    "\n",
    "start = time.time()\n",
    "elapsed_time = 0\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "\n",
    "    epoch_time = time.time() - start_time\n",
    "    elapsed_time += epoch_time\n",
    "    print('| Elapsed time : %d:%02d:%02d'  %(get_hms(elapsed_time)))\n",
    "\n",
    "print('\\n[Phase 4] : Testing model')\n",
    "print('* Test results : Acc@1 = %.2f%%' %(best_acc))\n",
    "end = time.time()\n",
    "print('| Total time : %d:%02d:%02d'  %(get_hms(end - start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ResNet(18,10)\n",
    "# model.load_state_dict(torch.load(models_dir + '/' + 'theta_best.pt'))\n",
    "\n",
    "# print(sum(p.numel() for p in model.parameters())/1000000, \"M parameters\")\n",
    "\n",
    "# TENT\n",
    "net = model_best\n",
    "net = tent.configure_model(net)\n",
    "params, param_names = tent.collect_params(net)\n",
    "# optimizer = torch.optim.Adam(params, lr=1e-1)\n",
    "optimizer = torch.optim.SGD(params, lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "tented_model = tent.Tent(net, optimizer)\n",
    "# tented_model.load_state_dict(torch.load(models_dir + '/' + 'theta_best.pt'))\n",
    "\n",
    "tented_model = tented_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model\n",
    "model = model_best\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "for batch_idx, (inputs, targets) in enumerate(valloader):\n",
    "    if use_cuda:\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "    with torch.no_grad():\n",
    "        inputs, targets = Variable(inputs), Variable(targets)\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    test_loss += loss.item()\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += targets.size(0)\n",
    "    correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "acc = (100.*correct/total).item()\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tented model\n",
    "\n",
    "tented_model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "for batch_idx, (inputs, targets) in enumerate(valloader):\n",
    "    if use_cuda:\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "    with torch.no_grad():\n",
    "        inputs, targets = Variable(inputs), Variable(targets)\n",
    "    outputs = tented_model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    test_loss += loss.item()\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += targets.size(0)\n",
    "    correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "acc = (100.*correct/total).item()\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10 Rotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rotated = np.load(\"data/CIFAR10_rotated.npy\")\n",
    "# data_rotated = np.transpose(data_rotated, (0,1,4,2,3))\n",
    "data_rotated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_flattened = []\n",
    "labels_flattened = []\n",
    "\n",
    "for n in range(16):\n",
    "    data_flattened.append(data_rotated[n])\n",
    "    labels_flattened.append(valset.test_labels)\n",
    "\n",
    "data_flattened = np.vstack(data_flattened)\n",
    "labels_flattened = np.hstack(labels_flattened)\n",
    "print(data_flattened.shape, labels_flattened.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dev = valset.test_data\n",
    "y_dev = valset.test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Get Tented Model\n",
    "model = ResNet(18,10)\n",
    "model.load_state_dict(torch.load(models_dir + '/' + 'theta_best.pt'))\n",
    "\n",
    "net = model\n",
    "net = tent.configure_model(net)\n",
    "params, param_names = tent.collect_params(net)\n",
    "optimizer = torch.optim.Adam(params, lr=1e-3)\n",
    "# optimizer = torch.optim.SGD(params, lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "tented_model = tent.Tent(net, optimizer)\n",
    "# tented_model.load_state_dict(torch.load(models_dir + '/' + 'theta_best.pt'))\n",
    "# tented_model = tented_model.to(device)\n",
    "tented_model.eval()\n",
    "\n",
    "\n",
    "im_ind = 23\n",
    "Nsamples = 100\n",
    "steps = 16\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow( ndim.interpolation.rotate(x_dev[im_ind], 0, reshape=False, mode='nearest'))\n",
    "plt.title('original image')\n",
    "plt.savefig(results_dir + '/sample_image.png', bbox_inches='tight')\n",
    "s_rot = 0\n",
    "end_rot = 179\n",
    "rotations = (np.linspace(s_rot, end_rot, steps)).astype(int)            \n",
    "\n",
    "ims = []\n",
    "predictions = []\n",
    "# percentile_dist_confidence = []\n",
    "x, y = x_dev[im_ind], y_dev[im_ind]\n",
    "\n",
    "fig = plt.figure(figsize=(steps, 8), dpi=80)\n",
    "\n",
    "ims = data_rotated[:,im_ind,:,:,:]\n",
    "# print(ims.shape)\n",
    "ims  =np.transpose(ims,(0,3,1,2))\n",
    "# print(ims.shape)\n",
    "\n",
    "ims = torch.Tensor(ims)\n",
    "logits = []\n",
    "for n in range(Nsamples):\n",
    "    outputs = tented_model(ims)\n",
    "    outputs = outputs.detach().cpu().numpy()\n",
    "    logits.append(outputs)\n",
    "\n",
    "logits = np.stack(logits)\n",
    "print(logits.shape)\n",
    "logits = np.mean(logits, axis=0)\n",
    "print(logits.shape)\n",
    "predictions = softmax(logits,1)\n",
    "\n",
    "textsize = 15\n",
    "lw = 5\n",
    "\n",
    "c = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd',\n",
    "    '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']  \n",
    "\n",
    "ax0 = plt.subplot2grid((3, steps-1), (0, 0), rowspan=2, colspan=steps-1)\n",
    "#ax0 = fig.add_subplot(2, 1, 1)\n",
    "plt.gca().set_prop_cycle(color = c)\n",
    "ax0.plot(rotations, predictions, linewidth=lw)\n",
    "\n",
    "\n",
    "##########################\n",
    "# Dots at max\n",
    "\n",
    "for i in range(predictions.shape[1]):\n",
    "\n",
    "    selections = (predictions[:,i] == predictions.max(axis=1))\n",
    "    for n in range(len(selections)):\n",
    "        if selections[n]:\n",
    "            ax0.plot(rotations[n], predictions[n, i], 'o', c=c[i], markersize=15.0)\n",
    "##########################  \n",
    "\n",
    "lgd = ax0.legend(['airplane', 'automobile', 'bird',\n",
    "            'cat', 'deer', 'dog',\n",
    "            'frog', 'horse', 'ship',\n",
    "            'truck'], loc='upper right', prop={'size': textsize, 'weight': 'normal'}, bbox_to_anchor=(1.35,1))\n",
    "plt.xlabel('rotation angle')\n",
    "# plt.ylabel('probability')\n",
    "plt.title('True class: %d, Nsamples %d' % (y, Nsamples))\n",
    "# ax0.axis('tight')\n",
    "plt.tight_layout()\n",
    "plt.autoscale(enable=True, axis='x', tight=True)\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "for item in ([ax0.title, ax0.xaxis.label, ax0.yaxis.label] +\n",
    "            ax0.get_xticklabels() + ax0.get_yticklabels()):\n",
    "    item.set_fontsize(textsize)\n",
    "    item.set_weight('normal')\n",
    "\n",
    "plt.savefig(results_dir + '/percentile_label_probabilities.png', bbox_extra_artists=(lgd,), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Get Tented Model\n",
    "model = ResNet(18,10)\n",
    "model.load_state_dict(torch.load(models_dir + '/' + 'theta_best.pt'))\n",
    "\n",
    "net = model\n",
    "net = tent.configure_model(net)\n",
    "params, param_names = tent.collect_params(net)\n",
    "optimizer = torch.optim.Adam(params, lr=1e-3)\n",
    "# optimizer = torch.optim.SGD(params, lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "tented_model = tent.Tent(net, optimizer)\n",
    "# tented_model.load_state_dict(torch.load(models_dir + '/' + 'theta_best.pt'))\n",
    "tented_model = tented_model.to(device)\n",
    "tented_model.eval()\n",
    "\n",
    "steps = 16\n",
    "N = 10000\n",
    "Nsamples = 100\n",
    "y_dev = valset.test_labels\n",
    "\n",
    "def preprocess_test(X):\n",
    "\n",
    "    N, H, W, C = X.shape\n",
    "    Y = torch.zeros(N, C, H, W)\n",
    "    mean = (0.4914, 0.4822, 0.4465)\n",
    "    std = (0.2023, 0.1994, 0.2010)  \n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean,std),\n",
    "    ])\n",
    "\n",
    "    for n in range(len(X)):\n",
    "        Y[n] =  transform_test(X[n])\n",
    "\n",
    "    return Y\n",
    "\n",
    "# for im_ind in range(N):\n",
    "#     if(im_ind % 500 == 0):\n",
    "#         print(im_ind)\n",
    "\n",
    "#     y =  y_dev[im_ind]\n",
    "    \n",
    "#     ims = data_rotated[:,im_ind,:,:,:]\n",
    "#     ims = preprocess_test(ims)\n",
    "#     # print(ims.shape)\n",
    "\n",
    "#     y = np.ones(ims.shape[0])*y\n",
    "    \n",
    "#     # sample_probs = tented_model.all_sample_eval(ims, torch.from_numpy(y), Nsamples=Nsamples)\n",
    "\n",
    "#     predictions = torch.zeros(Nsamples, steps, 10)\n",
    "\n",
    "#     ims = ims.to(device)\n",
    "#     for i in range(Nsamples):\n",
    "#         y = tented_model(ims)\n",
    "#         # print(y.shape)\n",
    "#         predictions[i] = y\n",
    "\n",
    "#     probs = F.softmax(predictions, dim=2)\n",
    "#     probs = probs.detach().numpy()\n",
    "\n",
    "#     all_sample_preds[im_ind] = probs\n",
    "#     all_preds[im_ind] = np.mean(probs, axis=0)\n",
    "\n",
    "preds_list = []\n",
    "preds_samples_list = []\n",
    "\n",
    "data_flattened = preprocess_test(data_flattened)\n",
    "labels_flattened = torch.Tensor(labels_flattened)\n",
    "labels_flattened = labels_flattened.long()\n",
    "\n",
    "chal_dataset = torch.utils.data.TensorDataset(data_flattened, labels_flattened)\n",
    "batch_size = 200\n",
    "chal_loader = torch.utils.data.DataLoader(chal_dataset, batch_size=batch_size)\n",
    "chal_error = 0\n",
    "\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for idx, data in enumerate(chal_loader):\n",
    "        if (idx % 100==0):\n",
    "            print(idx)\n",
    "        x,y = data\n",
    "        # cost, err, probs = net.sample_eval(x, y, Nsamples=10, logits=False)\n",
    "        # preds_list.append(probs.cpu().numpy())\n",
    "        # chal_error += err.cpu().numpy()\n",
    "        x = x.to(device)\n",
    "        predictions = torch.zeros(Nsamples, batch_size, 10)\n",
    "        for i in range(Nsamples):\n",
    "            outputs = tented_model(x)\n",
    "            # print(y.shape)\n",
    "            predictions[i] = F.softmax(outputs, dim=1)\n",
    "        \n",
    "        preds_samples_list.append(predictions.cpu().numpy())\n",
    "        preds = predictions.mean(0)\n",
    "        preds_list.append(preds.cpu().numpy())\n",
    "        correct += torch.sum(torch.argmax(preds,1) == y)\n",
    "    # print(err)\n",
    "\n",
    "# print(chal_error)\n",
    "acc = (correct/len(chal_dataset)).item()\n",
    "print(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_samples_list = np.concatenate(preds_samples_list,axis=1)\n",
    "preds_list = np.vstack(preds_list)\n",
    "print(preds_samples_list.shape, preds_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_samples_list = np.transpose(preds_samples_list,(1,0,2))\n",
    "preds_samples_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = np.zeros((N, steps, 10))\n",
    "all_sample_preds = np.zeros((N, Nsamples,steps, 10))\n",
    "\n",
    "for n in range(16):\n",
    "    all_preds[:,n,:] = preds_list[n*10000:(n+1)*10000]\n",
    "    all_sample_preds[:,:,n,:] = preds_samples_list[n*10000:(n+1)*10000]\n",
    "\n",
    "all_preds += 1e-12\n",
    "all_sample_preds += 1e-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotations = (np.linspace(0, 179, steps)).astype(int)\n",
    "\n",
    "correct_preds = np.zeros((N, steps))\n",
    "for i in range(N):\n",
    "    correct_preds[i,:] = all_preds[i,:,y_dev[i]]   \n",
    "\n",
    "np.save(results_dir+'/correct_preds.npy', correct_preds)\n",
    "np.save(results_dir+'/all_preds.npy', all_preds)\n",
    "np.save(results_dir+'/all_sample_preds.npy', all_sample_preds)\n",
    "\n",
    "plot_predictive_entropy(correct_preds, all_preds, rotations, results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(preds == valset.test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR10-C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ResNet(18,10).to(device)\n",
    "# model.load_state_dict(torch.load(models_dir + '/' + 'theta_best.pt'))\n",
    "# checkpoint = torch.load(models_dir + '/' + 'theta_best.t7')\n",
    "# model = checkpoint['net']\n",
    "\n",
    "chalPath = 'data/CIFAR-10-C/'\n",
    "chals = sorted(os.listdir(chalPath))\n",
    "\n",
    "chal_labels = valset.test_labels\n",
    "chal_labels = torch.Tensor(chal_labels)\n",
    "chal_labels = chal_labels.long()\n",
    "\n",
    "Nsamples = 10\n",
    "\n",
    "def preprocess_test(X):\n",
    "\n",
    "    N, H, W, C = X.shape\n",
    "    Y = torch.zeros(N, C, H, W)\n",
    "    mean = (0.4914, 0.4822, 0.4465)\n",
    "    std = (0.2023, 0.1994, 0.2010)  \n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean,std),\n",
    "    ])\n",
    "\n",
    "    for n in range(len(X)):\n",
    "        Y[n] =  transform_test(X[n])\n",
    "\n",
    "    return Y\n",
    "\n",
    "preds_list= []\n",
    "# net.eval()\n",
    "avg_list = []\n",
    "\n",
    "for challenge in range(len(chals)):\n",
    "    chal_data = np.load(chalPath + chals[challenge])\n",
    "    # chal_data = np.transpose(chal_data, (0,3,1,2))\n",
    "\n",
    "    avg = 0\n",
    "    for j in range(5):\n",
    "\n",
    "        # Get Tented Model\n",
    "        model = ResNet(18,10)\n",
    "        model.load_state_dict(torch.load(models_dir + '/' + 'theta_best.pt'))\n",
    "\n",
    "        net = model\n",
    "        net = tent.configure_model(net)\n",
    "        params, param_names = tent.collect_params(net)\n",
    "        optimizer = torch.optim.Adam(params, lr=1e-3)\n",
    "        # optimizer = torch.optim.SGD(params, lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "        tented_model = tent.Tent(net, optimizer)\n",
    "        # tented_model.load_state_dict(torch.load(models_dir + '/' + 'theta_best.pt'))\n",
    "        tented_model = tented_model.to(device)\n",
    "        tented_model.eval()\n",
    "\n",
    "        # Load CIFAR10-C Data\n",
    "        chal_temp_data = chal_data[j * 10000:(j + 1) * 10000]\n",
    "        chal_temp_data = preprocess_test(chal_temp_data)\n",
    "\n",
    "        chal_dataset = torch.utils.data.TensorDataset(chal_temp_data, chal_labels)\n",
    "        batch_size = 200\n",
    "        chal_loader = torch.utils.data.DataLoader(chal_dataset, batch_size=batch_size)\n",
    "        chal_error = 0\n",
    "\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in chal_loader:\n",
    "                # cost, err, probs = net.sample_eval(x, y, Nsamples=10, logits=False)\n",
    "                # preds_list.append(probs.cpu().numpy())\n",
    "                # chal_error += err.cpu().numpy()\n",
    "                x = x.to(device)\n",
    "                predictions = torch.zeros(Nsamples, batch_size, 10)\n",
    "                for i in range(Nsamples):\n",
    "                    outputs = tented_model(x)\n",
    "                    # print(y.shape)\n",
    "                    predictions[i] = F.softmax(outputs, dim=1)\n",
    "                \n",
    "                preds = predictions.mean(0)\n",
    "                preds_list.append(preds.cpu().numpy())\n",
    "                correct += torch.sum(torch.argmax(preds,1) == y)\n",
    "            # print(err)\n",
    "\n",
    "        # print(chal_error)\n",
    "        chal_acc = (correct/len(chal_dataset)).item()\n",
    "        avg += chal_acc\n",
    "        print(round(chal_acc,4))\n",
    "    \n",
    "    avg /= 5\n",
    "    avg_list.append(avg)\n",
    "    print(\"Average:\", round(avg,4),\" \", chals[challenge])\n",
    "\n",
    "print(\"Mean: \", np.mean(avg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_list = np.vstack(preds_list)\n",
    "np.save(results_dir+'/preds_CIFAR-10-C.npy', preds_list)\n",
    "np.save(results_dir+'/avg_list_CIFAR-10-C.npy', avg_list)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31b8918c388efabf28e06d7aa829f320b99f50374aee3ccc5c6fe8cd01c3acd4"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
