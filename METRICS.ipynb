{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from scipy.special import softmax\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "# from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "valset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=False,num_workers=3)\n",
    "\n",
    "x_dev = []\n",
    "y_dev = []\n",
    "for x, y in valloader:\n",
    "    x_dev.append(x.cpu().numpy())\n",
    "    y_dev.append(y.cpu().numpy())\n",
    "\n",
    "x_dev = np.concatenate(x_dev)\n",
    "y_dev = np.concatenate(y_dev)\n",
    "print(x_dev.shape)\n",
    "print(y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log likelihood\n",
    "def get_ll(preds, targets):\n",
    "    return np.log(1e-12 + preds[np.arange(len(targets)), targets]).mean()\n",
    "\n",
    "# def get_RMSE(preds, targets):\n",
    "#     pass\n",
    "\n",
    "# Brier score\n",
    "# gentler than log loss in penalizing inaccurate predictions.\n",
    "def get_brier(preds, targets):\n",
    "    one_hot_targets = np.zeros(preds.shape)\n",
    "    one_hot_targets[np.arange(len(targets)), targets] = 1.0\n",
    "    return np.mean(np.sum((preds - one_hot_targets) ** 2, axis=1))\n",
    "\n",
    "def get_accuracy(preds, targets):\n",
    "    yhat = np.argmax(preds, 1)\n",
    "    accuracy = np.mean(yhat==targets)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_name_list = [\"Results/Regular_results\", \"Results/MCdrop_results\", \"Results/Ensemble_results\", \"Results/BBP_results/bbb\",\n",
    "                 \"Results/BBP_results/lrt\", \"Results/Contrastive_Reasoning_results\", \"Results/Tent_results\"]\n",
    "names = [\"Regular\", 'MC Dropout', 'Bootstrap Ensemble', 'BBP: without lrt' , 'BBP: with lrt', 'Contrastive Reasoning', 'TENT']\n",
    "\n",
    "metrics = np.zeros((len(result_name_list),3))\n",
    "\n",
    "targets = y_dev\n",
    "\n",
    "for idx, dir_name in enumerate(result_name_list):\n",
    "\n",
    "    all_preds = np.load(dir_name + '/all_preds.npy')\n",
    "    preds = all_preds[:, 0, :] # Just take at rotation 0\n",
    "\n",
    "    ll = get_ll(preds, targets)\n",
    "    brier = get_brier(preds, targets)\n",
    "    acc = get_accuracy(preds, targets)\n",
    "    \n",
    "    metrics[idx,:] = ll,brier,acc\n",
    "\n",
    "pd.DataFrame(metrics, columns=[\"Log-likelihood\", \"Brier score\", \"Accuracy\"], index=names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10 rotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_name_list = [\"Results/Regular_results\", \"Results/MCdrop_results\", \"Results/Ensemble_results\", \"Results/BBP_results/bbb\",\n",
    "                 \"Results/BBP_results/lrt\", \"Results/Contrastive_Reasoning_results\", \"Results/Tent_results\"]\n",
    "names = [\"Regular\", 'MC Dropout', 'Bootstrap Ensemble', 'BBP: without lrt' , 'BBP: with lrt', 'Contrastive Reasoning', 'TENT']\n",
    "\n",
    "metrics = np.zeros((len(result_name_list),3))\n",
    "\n",
    "targets = np.repeat(y_dev,16)\n",
    "\n",
    "for idx, dir_name in enumerate(result_name_list):\n",
    "\n",
    "    all_preds = np.load(dir_name + '/all_preds.npy')\n",
    "    N, R, C = all_preds.shape\n",
    "    preds = all_preds.reshape(-1, C) # Over all rotations\n",
    "\n",
    "    ll = get_ll(preds, targets)\n",
    "    brier = get_brier(preds, targets)\n",
    "    acc = get_accuracy(preds, targets)\n",
    "    \n",
    "    metrics[idx,:] = ll,brier,acc\n",
    "\n",
    "pd.DataFrame(metrics, columns=[\"Log-likelihood\", \"Brier score\", \"Accuracy\"], index=names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10-C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = []\n",
    "# for key in preds_dict.keys():\n",
    "#     labels.append(key.split(\".\")[0])\n",
    "# print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Boxplot of metrics across distortion types\n",
    "\n",
    "# plt.figure(figsize=(25,10))\n",
    "# ax = plt.boxplot(ll_list, labels=labels)\n",
    "# plt.xlabel(\"Distortion type\"); plt.ylabel(\"Log likelihood\")\n",
    "# plt.grid()\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(25,10))\n",
    "# plt.boxplot(brier_list, labels=labels)\n",
    "# plt.xlabel(\"Distortion type\"); plt.ylabel(\"Brier score\")\n",
    "# plt.grid()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_name_list = [\"Results/Regular_results\", \"Results/MCdrop_results\", \"Results/Ensemble_results\", \"Results/BBP_results/bbb\",\n",
    "                 \"Results/BBP_results/lrt\", \"Results/Contrastive_Reasoning_results\", \"Results/Tent_results\"]\n",
    "names = [\"Regular\", 'MC Dropout', 'Bootstrap Ensemble', 'BBP: without lrt' , 'BBP: with lrt', 'Contrastive Reasoning', 'TENT']\n",
    "\n",
    "metrics = np.zeros((len(result_name_list),3))\n",
    "targets = np.tile(y_dev,5*19)\n",
    "\n",
    "for idx, dir_name in enumerate(result_name_list):\n",
    "\n",
    "    preds = np.load(dir_name + '/preds_CIFAR-10-C.npy')\n",
    "\n",
    "    ll = get_ll(preds, targets)\n",
    "    brier = get_brier(preds, targets)\n",
    "    acc = get_accuracy(preds, targets)\n",
    "    \n",
    "    metrics[idx,:] = ll,brier,acc\n",
    "\n",
    "pd.DataFrame(metrics, columns=[\"Log-likelihood\", \"Brier score\", \"Accuracy\"], index=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eed44ad03d3398d64d3d4bda079817b6c24e225a548088d53b43c511791e0bd9"
  },
  "kernelspec": {
   "display_name": "Python 3.6.8 ('megabayes')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
