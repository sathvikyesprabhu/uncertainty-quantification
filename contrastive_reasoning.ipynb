{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms, datasets\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from scipy.special import softmax\n",
    "\n",
    "from networks import ResNet\n",
    "from utils import *\n",
    "from utils_plotting import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "mean = (0.4914, 0.4822, 0.4465)\n",
    "std = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "transform_train = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.RandomCrop(32, padding=4),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean, std)\n",
    "                                    ])\n",
    "transform_test = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean, std)])\n",
    "\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "valset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "print(trainset.train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = \"Models/\" + 'Contrastive_Reasoning_models/'\n",
    "results_dir = \"Results/\" + 'Contrastive_Reasoning_results/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_classifier, self).__init__()\n",
    "        # my network is composed of only affine layers\n",
    "        self.bn = nn.BatchNorm1d(640)\n",
    "        self.fc1 = nn.Linear(640, 300)\n",
    "        self.fc1_drop = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(300, 100)\n",
    "        self.fc2_drop = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.bn(x)\n",
    "        # x = F.sigmoid(self.fc1(x))\n",
    "        # x = F.sigmoid(self.fc2(x))\n",
    "        x = ((torch.sigmoid(self.fc1(x))))\n",
    "        x = ((torch.sigmoid(self.fc2(x))))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ResNet(18, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cuda:\n",
    "    net.cuda()\n",
    "    net = torch.nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "def learning_rate(init, epoch):\n",
    "    optim_factor = 0\n",
    "    if (epoch > 300):\n",
    "        optim_factor = 4\n",
    "    if(epoch > 160):\n",
    "        optim_factor = 3\n",
    "    elif(epoch > 120):\n",
    "        optim_factor = 2\n",
    "    elif(epoch > 60):\n",
    "        optim_factor = 1\n",
    "\n",
    "    return init*math.pow(0.2, optim_factor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(epoch): \n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    lr = 0.1\n",
    "    \n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate(lr, epoch), momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "    print('\\n=> Training Epoch #%d, LR=%.4f' %(epoch, learning_rate(lr, epoch)))\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda() # GPU settings\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = Variable(inputs), Variable(targets)\n",
    "        outputs = net(inputs)               # Forward Propagation\n",
    "        loss = criterion(outputs, targets)  # Loss\n",
    "        loss.backward()  # Backward Propagation\n",
    "        optimizer.step() # Optimizer update\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "        # sys.stdout.write('\\r')\n",
    "        # sys.stdout.write('| Epoch [%3d/%3d] Iter[%3d/%3d]\\t\\tLoss: %.4f Acc@1: %.3f%%'\n",
    "        #         %(epoch, num_epochs, batch_idx+1,\n",
    "        #             (len(trainset)//batch_size)+1, loss.item(), 100.*correct/total))\n",
    "        # sys.stdout.flush()\n",
    "    print('| Epoch [%3d/%3d] Iter[%3d/%3d]\\t\\tLoss: %.4f Acc@1: %.3f%%'\n",
    "                %(epoch, num_epochs, batch_idx+1,\n",
    "                    (len(trainset)//batch_size)+1, loss.item(), 100.*correct/total))\n",
    "\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(valloader):\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        with torch.no_grad():\n",
    "            inputs, targets = Variable(inputs), Variable(targets)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "    # Save checkpoint when best model\n",
    "    acc = 100.*correct/total\n",
    "    print(\"\\n| Validation Epoch #%d\\t\\t\\tLoss: %.4f Acc@1: %.2f%%\" %(epoch, loss.item(), acc))\n",
    "\n",
    "    if acc > best_acc:\n",
    "        print('| Saving Best model...\\t\\t\\tTop1 = %.2f%%' %(acc))\n",
    "        state = {\n",
    "                'net':net.module if use_cuda else net,\n",
    "                'acc':acc,\n",
    "                'epoch':epoch,\n",
    "        }\n",
    "\n",
    "        torch.save(state, models_dir + '/' + 'backbone.t7')\n",
    "        best_acc = acc\n",
    "\n",
    "def get_hms(seconds):\n",
    "    m, s = divmod(seconds, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "\n",
    "    return h, m, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "best_acc = 0\n",
    "\n",
    "start = time.time()\n",
    "elapsed_time = 0\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "\n",
    "    epoch_time = time.time() - start_time\n",
    "    elapsed_time += epoch_time\n",
    "    print('| Elapsed time : %d:%02d:%02d'  %(get_hms(elapsed_time)))\n",
    "\n",
    "print('\\nTesting model')\n",
    "print('* Test results : Acc@1 = %.2f%%' %(best_acc))\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Projection Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grads(loader, model, data, feat):  # , target_class):\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    ce_loss2 = nn.MSELoss()\n",
    "\n",
    "    num_images = len(data)\n",
    "    all_grads = []\n",
    "    all_grad_pred = []\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    time_avg = 0\n",
    "\n",
    "    for num_im in range(num_images):\n",
    "\n",
    "        # print(num_im)\n",
    "\n",
    "        img = loader(data[num_im])\n",
    "        img = torch.unsqueeze(img, 0)\n",
    "\n",
    "        img = img.cuda()\n",
    "        grad = torch.zeros(0)\n",
    "        grad_pred = torch.zeros(0)\n",
    "        #start_time = time.time()\n",
    "\n",
    "\n",
    "        # im_label_as_var = torch.from_numpy(np.asarray([target_label]))\n",
    "        im_label_as_var2 = torch.from_numpy(feat*np.ones(10)).float()\n",
    "        #im_label_as_var2[target_label] = feat\n",
    "        # im_label_as_var2 = torch.unsqueeze(im_label_as_var2, 0)\n",
    "\n",
    "        output = model(img)[0]\n",
    "\n",
    "        # pred_loss = FocalLoss(gamma=2)(output.cuda(), im_label_as_var.cuda())\n",
    "        pred_loss = ce_loss2(output.cuda(), im_label_as_var2.cuda())\n",
    "        pred_loss.backward(retain_graph=False)\n",
    "\n",
    "        temp_grad = model.linear.weight.grad\n",
    "\n",
    "        #temp_grad = temp_grad[target_label]\n",
    "        temp_grad = torch.unsqueeze(temp_grad, 0)\n",
    "        temp_grad = temp_grad.data.cpu()\n",
    "        temp_grad = torch.reshape(temp_grad, (1, 640))\n",
    "        #plt.imshow(temp_grad.cpu().squeeze().numpy())\n",
    "        #plt.show()\n",
    "        grad = torch.cat((grad, temp_grad), 1)\n",
    "\n",
    "        model.linear.weight.grad.zero_()\n",
    "        model.linear.bias.grad.zero_()\n",
    "        # model.param.grad.zero_()\n",
    "\n",
    "        del temp_grad, pred_loss, im_label_as_var2, output\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "        del img\n",
    "        torch.cuda.empty_cache()\n",
    "        grad = grad.squeeze(0)\n",
    "\n",
    "        all_grads.append(grad.cpu())\n",
    "        all_grad_pred.append(grad_pred.cpu())\n",
    "\n",
    "        del grad, grad_pred\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    #average = time_avg / num_images\n",
    "    #print(average)\n",
    "    return all_grads, all_grad_pred\n",
    "\n",
    "def scale(X, x_min, x_max):\n",
    "    nom = (X - X.min(axis=0)) * (x_max - x_min)\n",
    "    denom = X.max(axis=0) - X.min(axis=0)\n",
    "    denom[denom == 0] = 1\n",
    "    return x_min + nom / denom\n",
    "\n",
    "def preprocess_test(dat1):\n",
    "    data_np1 = [(np.asarray(i)) for i in dat1]\n",
    "    data_np1 = np.asarray(data_np1)\n",
    "    data_np1 = data_np1.squeeze()\n",
    "\n",
    "    data = scale(data_np1, -1, 1)\n",
    "\n",
    "    return data\n",
    "\n",
    "def preprocess_train(dat1):\n",
    "    data_np1 = [(np.asarray(i)) for i in dat1]\n",
    "    data_np1 = np.asarray(data_np1)\n",
    "    data_np1 = data_np1.squeeze()\n",
    "\n",
    "    data_np1 = scale(data_np1, -1, 1)\n",
    "\n",
    "    return data_np1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # Resnet-50,101 = 2560\n",
    "        data = data.view(-1, 640)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        # loss = F.nll_loss(output, target)\n",
    "        # loss = F.kl_div(output, target) #nn.KLDivLoss(output, target)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 1000 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test(net, testloader):\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    logits = []\n",
    "    labels = []\n",
    "    preds = []\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        # targets = torch.unsqueeze(targets,1)\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        with torch.no_grad():\n",
    "            inputs, targets = Variable(inputs), Variable(targets)\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        logits.append(outputs.tolist())\n",
    "        labels.append(targets.tolist())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        preds.append(predicted)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "    acc = 100. * correct / total\n",
    "    return acc, correct, total, logits, labels, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(models_dir + '/' + 'backbone.t7')\n",
    "model = checkpoint['net']\n",
    "model = model.cuda()\n",
    "\n",
    "test_labels = valset.test_labels\n",
    "test_labels = torch.Tensor(test_labels)\n",
    "test_labels = test_labels.long()\n",
    "\n",
    "train_labels = trainset.train_labels\n",
    "train_labels = torch.Tensor(train_labels)\n",
    "train_labels = train_labels.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get gradients\n",
    "grads_train, _ = get_grads(transform_test, model, trainset.train_data, feat= 15.5)\n",
    "grads_test, _ = get_grads(transform_test, model, valset.test_data, feat = 15.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_grads = preprocess_train(grads_train)\n",
    "train_tensor = torch.stack([torch.Tensor(i) for i in train_grads])\n",
    "train_dataset = torch.utils.data.TensorDataset(train_tensor, train_labels)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "test_grads = preprocess_test(grads_test)\n",
    "test_tensor = torch.stack([torch.Tensor(i) for i in test_grads])\n",
    "test_dataset = torch.utils.data.TensorDataset(test_tensor, test_labels)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "lr = 0.1\n",
    "\n",
    "net = Net_classifier().cuda()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate(lr, epoch), momentum=0.9, weight_decay=5e-4)\n",
    "    train(net, device, train_loader, optimizer, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, correct, total, logits, labels, preds = test(net, test_loader)\n",
    "print(correct.numpy() / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net, models_dir + '/' + 'head.t7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(models_dir + '/' + 'backbone.t7')\n",
    "model = checkpoint['net']\n",
    "\n",
    "mlp = torch.load(models_dir + '/' + 'head.t7')\n",
    "mlp = mlp.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = valset.test_labels\n",
    "test_labels = torch.Tensor(test_labels)\n",
    "test_labels = test_labels.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valset.test_data[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get gradients\n",
    "grads_test, _ = get_grads(transform_test, model, valset.test_data, feat = 15.5)\n",
    "\n",
    "test_grads = preprocess_test(grads_test)\n",
    "test_tensor = torch.stack([torch.Tensor(i) for i in test_grads])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = torch.utils.data.TensorDataset(test_tensor, test_labels)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, testloader):\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    logits = []\n",
    "    labels = []\n",
    "    preds = []\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        # targets = torch.unsqueeze(targets,1)\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        with torch.no_grad():\n",
    "            inputs, targets = Variable(inputs), Variable(targets)\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        logits.append(outputs.tolist())\n",
    "        labels.append(targets.tolist())\n",
    "        # print(np.shape(logits), np.shape(labels))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        preds.append(predicted)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "    acc = 100. * correct / total\n",
    "    return acc, correct, total, logits, labels, preds\n",
    "\n",
    "acc, correct, total, logits, labels, preds = test(mlp, test_loader)\n",
    "print(correct.numpy() / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(t):\n",
    "    return [item for sublist in t for item in sublist]\n",
    "    \n",
    "all_preds =  np.array(flatten(logits))\n",
    "all_preds = softmax(all_preds,1)\n",
    "all_preds = np.expand_dims(all_preds,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(results_dir + '/' + 'all_preds.npy',all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GradCon_Net(nn.Module):\n",
    "#     def __init__(self, model):\n",
    "#         self.model = model\n",
    "\n",
    "#     def sample_predict(self, x, Nsamples):  # Nsamples: No. of forward passes\n",
    "#         predictions = x.data.new(Nsamples, x.shape[0], self.num_classes)\n",
    "#         for i in range(Nsamples):\n",
    "#             y,_= self.forward(x)\n",
    "#             predictions[i] = y\n",
    "\n",
    "#         return predictions\n",
    "\n",
    "#     def sample_eval(self, x, y, Nsamples, logits=True, train=False):\n",
    "#         x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n",
    "#         out = self.model.sample_predict(x, Nsamples)\n",
    "\n",
    "#         if logits:\n",
    "#             mean_out = out.mean(dim=0, keepdim=False)\n",
    "#             loss = F.cross_entropy(mean_out, y, reduction='sum')\n",
    "#             probs = F.softmax(mean_out, dim=1).data.cpu()\n",
    "\n",
    "#         else:\n",
    "#             mean_out = F.softmax(out, dim=2).mean(dim=0, keepdim=False)\n",
    "#             probs = mean_out.data.cpu()\n",
    "\n",
    "#             log_mean_probs_out = torch.log(mean_out)\n",
    "#             loss = F.nll_loss(log_mean_probs_out, y, reduction='sum')\n",
    "\n",
    "#         pred = mean_out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n",
    "#         err = pred.ne(y.data).sum()\n",
    "\n",
    "#         return loss.data, err, probs\n",
    "\n",
    "#     def all_sample_eval(self, x, y, Nsamples):\n",
    "#         \"\"\"Returns predictions for sample\"\"\"\n",
    "#         x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n",
    "\n",
    "#         out = self.model.sample_predict(x, Nsamples)\n",
    "\n",
    "#         prob_out = F.softmax(out, dim=2)\n",
    "#         prob_out = prob_out.data\n",
    "\n",
    "#         return prob_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10-C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grads(loader, model, data, feat):  # , target_class):\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    ce_loss2 = nn.MSELoss()\n",
    "\n",
    "    num_images = len(data)\n",
    "    all_grads = []\n",
    "    all_grad_pred = []\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    time_avg = 0\n",
    "\n",
    "    for num_im in range(num_images):\n",
    "\n",
    "        # print(num_im)\n",
    "\n",
    "        img = loader(data[num_im])\n",
    "        img = torch.unsqueeze(img, 0)\n",
    "\n",
    "        img = img.cuda()\n",
    "        grad = torch.zeros(0)\n",
    "        grad_pred = torch.zeros(0)\n",
    "        #start_time = time.time()\n",
    "\n",
    "\n",
    "        # im_label_as_var = torch.from_numpy(np.asarray([target_label]))\n",
    "        im_label_as_var2 = torch.from_numpy(feat*np.ones(10)).float() #vector of ones for training labels\n",
    "        #im_label_as_var2[target_label] = feat\n",
    "        im_label_as_var2 = torch.unsqueeze(im_label_as_var2, 0)\n",
    "\n",
    "        output = model(img)[0]\n",
    "        output = torch.unsqueeze(output, 0)\n",
    "\n",
    "        # print(output.shape, im_label_as_var2.shape)\n",
    "        # pred_loss = FocalLoss(gamma=2)(output.cuda(), im_label_as_var.cuda())\n",
    "        pred_loss = ce_loss2(output.cuda(), im_label_as_var2.cuda())\n",
    "        pred_loss.backward(retain_graph=False)\n",
    "\n",
    "        temp_grad = model.linear.weight.grad\n",
    "\n",
    "        #temp_grad = temp_grad[target_label]\n",
    "        temp_grad = torch.unsqueeze(temp_grad, 0)\n",
    "        temp_grad = temp_grad.data.cpu()\n",
    "        temp_grad = torch.reshape(temp_grad, (1, 640))\n",
    "        #plt.imshow(temp_grad.cpu().squeeze().numpy())\n",
    "        #plt.show()\n",
    "        grad = torch.cat((grad, temp_grad), 1)\n",
    "\n",
    "        model.linear.weight.grad.zero_()\n",
    "        model.linear.bias.grad.zero_()\n",
    "        # model.param.grad.zero_()\n",
    "\n",
    "        del temp_grad, pred_loss, im_label_as_var2, output\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "        del img\n",
    "        torch.cuda.empty_cache()\n",
    "        grad = grad.squeeze(0)\n",
    "\n",
    "        all_grads.append(grad.cpu())\n",
    "        all_grad_pred.append(grad_pred.cpu())\n",
    "\n",
    "        del grad, grad_pred\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    #average = time_avg / num_images\n",
    "    #print(average)\n",
    "    return all_grads, all_grad_pred\n",
    "\n",
    "def scale(X, x_min, x_max):\n",
    "    nom = (X - X.min(axis=0)) * (x_max - x_min)\n",
    "    denom = X.max(axis=0) - X.min(axis=0)\n",
    "    denom[denom == 0] = 1\n",
    "    return x_min + nom / denom\n",
    "\n",
    "def preprocess_test(dat1):\n",
    "    data_np1 = [(np.asarray(i)) for i in dat1]\n",
    "    data_np1 = np.asarray(data_np1)\n",
    "    data_np1 = data_np1.squeeze()\n",
    "\n",
    "    data = scale(data_np1, -1, 1)\n",
    "\n",
    "    return data\n",
    "\n",
    "def test(net, testloader):\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    preds = []\n",
    "    labels = []\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        # targets = torch.unsqueeze(targets,1)\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        with torch.no_grad():\n",
    "            inputs, targets = Variable(inputs), Variable(targets)\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        preds.append(outputs.tolist())\n",
    "        labels.append(targets.tolist())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "    # Save checkpoint when best model\n",
    "    acc = 100. * correct / total\n",
    "    return acc, correct, total, preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(models_dir + '/' + 'backbone.t7')\n",
    "model = checkpoint['net']\n",
    "\n",
    "mlp = torch.load(models_dir + '/' + 'head.t7')\n",
    "mlp = mlp.cuda()\n",
    "\n",
    "chalPath = 'data/CIFAR-10-C/'\n",
    "chals = os.listdir(chalPath)\n",
    "chals.sort()\n",
    "\n",
    "mean = (0.4914, 0.4822, 0.4465)\n",
    "std = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean,std),\n",
    "])\n",
    "\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "test_labels = testset.test_labels\n",
    "test_labels = torch.Tensor(test_labels)\n",
    "test_labels = test_labels.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_list = []\n",
    "avg_list = []\n",
    "\n",
    "for challenge in range(len(chals)):\n",
    "    \n",
    "    path = chalPath + chals[challenge]\n",
    "    chal_data = np.load(path)\n",
    "    #acts_test = get_acts(model, chal_data, 0)\n",
    "    grads_s, _ = get_grads(transform_test, model, chal_data, feat=15.5)\n",
    "    avg = 0\n",
    "\n",
    "    for j in range(5):\n",
    "\n",
    "        chal_temp_data = grads_s[j * 10000:(j + 1) * 10000]\n",
    "        chal_grads = preprocess_test(chal_temp_data)\n",
    "        chal_tensor = torch.stack([torch.Tensor(i) for i in chal_grads])\n",
    "        chal_dataset = torch.utils.data.TensorDataset(chal_tensor, test_labels)\n",
    "        chal_loader = torch.utils.data.DataLoader(chal_dataset, batch_size=10000)\n",
    "\n",
    "        _, correct, total, preds, labels = test(mlp, chal_loader)\n",
    "        preds_list.append(preds[0])\n",
    "        print(correct.numpy() / total)\n",
    "        avg = avg + (correct.numpy() / total)\n",
    "\n",
    "    avg = avg / 5\n",
    "    avg_list.append(avg)\n",
    "    print(\"Average:\",avg,\" \", chals[challenge])\n",
    "\n",
    "print(\"Mean: \", np.mean(avg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_list = np.vstack(preds_list)\n",
    "print(preds_list.shape)\n",
    "preds_list1 = softmax(preds_list,1)\n",
    "np.save(results_dir+'/preds_CIFAR-10-C.npy', preds_list1)\n",
    "np.save(results_dir+'/avg_list_CIFAR-10-C.npy', avg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10 Rotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dev = []\n",
    "y_dev = []\n",
    "for x, y in valloader:\n",
    "    x_dev.append(x.cpu().numpy())\n",
    "    y_dev.append(y.cpu().numpy())\n",
    "\n",
    "x_dev = np.concatenate(x_dev)\n",
    "y_dev = np.concatenate(y_dev)\n",
    "\n",
    "x_dev = np.transpose(x_dev, (0,2,3,1))\n",
    "print(x_dev.shape)\n",
    "print(y_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dev = valset.test_data\n",
    "print(x_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rotated = np.load(\"data/CIFAR10_rotated.npy\")\n",
    "# data_rotated = np.transpose(data_rotated, (0,1,4,2,3))\n",
    "data_rotated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, testloader):\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    logits = []\n",
    "    labels = []\n",
    "    preds = []\n",
    "\n",
    "    for inputs, targets in testloader:\n",
    "        # targets = torch.unsqueeze(targets,1)\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        with torch.no_grad():\n",
    "            inputs, targets = Variable(inputs), Variable(targets)\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        logits.append(outputs.tolist())\n",
    "        labels.append(targets.tolist())\n",
    "        # print(np.shape(logits), np.shape(labels))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        preds.append(predicted)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "    acc = (100. * correct / total).item()\n",
    "    return acc, logits, labels, preds\n",
    "\n",
    "\n",
    "im_ind = 23\n",
    "# Nsamples = 1\n",
    "steps = 16\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow( ndim.interpolation.rotate(x_dev[im_ind,:,:,:], 0, reshape=False, mode='nearest'))\n",
    "plt.title('original image')\n",
    "plt.savefig(results_dir + '/sample_image.png', bbox_inches='tight')\n",
    "s_rot = 0\n",
    "end_rot = 179\n",
    "rotations = (np.linspace(s_rot, end_rot, steps)).astype(int)            \n",
    "\n",
    "ims = []\n",
    "predictions = []\n",
    "# percentile_dist_confidence = []\n",
    "x, y = x_dev[im_ind], y_dev[im_ind]\n",
    "\n",
    "fig = plt.figure(figsize=(steps, 8), dpi=80)\n",
    "\n",
    "ims = data_rotated[:,im_ind,:,:,:]\n",
    "print(ims.shape)\n",
    "test_labels = np.ones(ims.shape[0])*y\n",
    "y = test_labels\n",
    "test_labels = torch.Tensor(test_labels)\n",
    "test_labels = test_labels.long()\n",
    "\n",
    "grads_test, _ = get_grads(transform_test, model, ims , feat = 15.5)\n",
    "test_grads = preprocess_test(grads_test)\n",
    "test_tensor = torch.stack([torch.Tensor(i) for i in test_grads])\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(test_tensor, test_labels)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=steps)\n",
    "\n",
    "acc, logits, labels, preds = test(mlp, test_loader)\n",
    "print(acc)\n",
    "\n",
    "logits = np.vstack(logits)\n",
    "predictions = softmax(logits,1)\n",
    "\n",
    "textsize = 15\n",
    "lw = 5\n",
    "\n",
    "c = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd',\n",
    "    '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']  \n",
    "\n",
    "ax0 = plt.subplot2grid((3, steps-1), (0, 0), rowspan=2, colspan=steps-1)\n",
    "#ax0 = fig.add_subplot(2, 1, 1)\n",
    "plt.gca().set_prop_cycle(color = c)\n",
    "ax0.plot(rotations, predictions, linewidth=lw)\n",
    "\n",
    "\n",
    "##########################\n",
    "# Dots at max\n",
    "\n",
    "for i in range(predictions.shape[1]):\n",
    "\n",
    "    selections = (predictions[:,i] == predictions.max(axis=1))\n",
    "    for n in range(len(selections)):\n",
    "        if selections[n]:\n",
    "            ax0.plot(rotations[n], predictions[n, i], 'o', c=c[i], markersize=15.0)\n",
    "##########################  \n",
    "\n",
    "lgd = ax0.legend(['airplane', 'automobile', 'bird',\n",
    "            'cat', 'deer', 'dog',\n",
    "            'frog', 'horse', 'ship',\n",
    "            'truck'], loc='upper right', prop={'size': textsize, 'weight': 'normal'}, bbox_to_anchor=(1.35,1))\n",
    "plt.xlabel('rotation angle')\n",
    "# plt.ylabel('probability')\n",
    "plt.title('True class: %d' % (y[0]))\n",
    "# ax0.axis('tight')\n",
    "plt.tight_layout()\n",
    "plt.autoscale(enable=True, axis='x', tight=True)\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "for item in ([ax0.title, ax0.xaxis.label, ax0.yaxis.label] +\n",
    "            ax0.get_xticklabels() + ax0.get_yticklabels()):\n",
    "    item.set_fontsize(textsize)\n",
    "    item.set_weight('normal')\n",
    "\n",
    "# plt.savefig(results_dir + '/percentile_label_probabilities.png', bbox_extra_artists=(lgd,), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rotated = np.load(\"data/CIFAR10_rotated.npy\")\n",
    "data_rotated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, testloader):\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    logits = []\n",
    "    labels = []\n",
    "    preds = []\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        # targets = torch.unsqueeze(targets,1)\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        with torch.no_grad():\n",
    "            inputs, targets = Variable(inputs), Variable(targets)\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        logits.append(outputs.tolist())\n",
    "        labels.append(targets.tolist())\n",
    "        # print(np.shape(logits), np.shape(labels))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        preds.append(predicted)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "    logits = np.vstack(logits)\n",
    "    acc = (100. * correct / total).item()\n",
    "    return acc, logits, labels, preds\n",
    "\n",
    "N = 10000\n",
    "steps = 16\n",
    "all_preds = np.zeros((N, steps, 10))\n",
    "correct_preds = np.zeros((N, steps))\n",
    "test_labels = torch.Tensor(valset.test_labels)\n",
    "test_labels = test_labels.long()\n",
    "\n",
    "for r in range(steps):\n",
    "    # get gradients\n",
    "    grads_test, _ = get_grads(transform_test, model, data_rotated[r] , feat = 15.5)\n",
    "\n",
    "    test_grads = preprocess_test(grads_test)\n",
    "    test_tensor = torch.stack([torch.Tensor(i) for i in test_grads])\n",
    "\n",
    "    test_dataset = torch.utils.data.TensorDataset(test_tensor, test_labels)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    acc, logits, labels, preds = test(mlp, test_loader)\n",
    "    print(r,acc)\n",
    "\n",
    "    probs = softmax(logits,1)\n",
    "    all_preds[:,r,:] = probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotations = (np.linspace(0, 179, steps)).astype(int)\n",
    "\n",
    "y_dev = testset.test_labels\n",
    "correct_preds = np.zeros((N, steps))\n",
    "for i in range(N):\n",
    "    correct_preds[i,:] = all_preds[i,:,y_dev[i]]   \n",
    "\n",
    "plot_predictive_entropy(correct_preds, all_preds, rotations, results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(results_dir+'/correct_preds.npy', correct_preds)\n",
    "np.save(results_dir+'/all_preds.npy', all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000; Nsamples = 100; R=16; C=10\n",
    "all_sample_preds = np.zeros((N, Nsamples, R, C))\n",
    "\n",
    "for n in range(N):\n",
    "    for r in range(R):\n",
    "        for c in range(C):\n",
    "            all_sample_preds[n,:,r,c] = all_preds[n,r,c]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(results_dir+'/all_sample_preds.npy', all_sample_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31b8918c388efabf28e06d7aa829f320b99f50374aee3ccc5c6fe8cd01c3acd4"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
