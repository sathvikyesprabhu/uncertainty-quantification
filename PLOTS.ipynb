{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "from utils_plotting import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular NN\n",
    "\n",
    "batch_size = 128\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "valset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=False,num_workers=3)\n",
    "\n",
    "x_dev = []\n",
    "y_dev = []\n",
    "for x, y in valloader:\n",
    "    x_dev.append(x.cpu().numpy())\n",
    "    y_dev.append(y.cpu().numpy())\n",
    "\n",
    "x_dev = np.concatenate(x_dev)\n",
    "y_dev = np.concatenate(y_dev)\n",
    "print(x_dev.shape)\n",
    "print(y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicitve accuracy VS Rotation angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_name_list = [\"Results/Regular_results\"]\n",
    "\n",
    "correct_preds = np.load(result_name_list[0] + '/correct_preds.npy')\n",
    "all_preds = np.load(result_name_list[0] + '/all_preds.npy')\n",
    "\n",
    "pred_entropy = -(all_preds * np.log(all_preds)).sum(axis=2)\n",
    "pred_entropy_mean = pred_entropy.mean(axis=0)\n",
    "pred_entropy_std = pred_entropy.std(axis=0)\n",
    "\n",
    "# Get correct prediction probabilities\n",
    "correct_mean = correct_preds.mean(axis=0)\n",
    "correct_std = correct_preds.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_rot = 0\n",
    "end_rot = 179\n",
    "steps = 16\n",
    "rotations = (np.linspace(s_rot, end_rot, steps)).astype(int)\n",
    "\n",
    "fs = 25\n",
    "\n",
    "# plt.figure(dpi=100)\n",
    "# ax = plt.gca()\n",
    "fig = plt.figure(figsize=(steps, 12), dpi=80)\n",
    "\n",
    "ax = plt.subplot2grid((3, steps-1), (0, 0), rowspan=2, colspan=steps-1)\n",
    "\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "\n",
    "aa, = errorfill(rotations, correct_mean, yerr=correct_std, color=c[2], alpha_fill=0.25, ax=ax)\n",
    "\n",
    "#     ax = plt.gca()\n",
    "ax.set_xlabel('rotation angle')\n",
    "ax.set_ylabel('predictive accuracy')\n",
    "ax.set_title('MAP')\n",
    "ax2.yaxis.grid()\n",
    "ax.xaxis.grid() \n",
    "\n",
    "\n",
    "ax2.set_ylabel('nats')\n",
    "# bb, = errorfill(rotations, total_entropy_mean, yerr=total_entropy_std, color=c[0], ax=ax2)\n",
    "cc, = errorfill(rotations, pred_entropy_mean, yerr=pred_entropy_std, color=c[1], ax=ax2)\n",
    "#     dd, = errorfill(rotations, epistemic_entropy_mean, yerr=epistemic_entropy_std, color=c[3], ax=ax2)\n",
    "\n",
    "\n",
    "# lgd = plt.legend(['correct class', 'posterior predictive entropy'], loc='upper right',\n",
    "#                  prop={'size': 15, 'weight': 'normal'}, bbox_to_anchor=(1.4,1))\n",
    "\n",
    "p_lgd = [aa, cc]\n",
    "lgd = plt.legend(p_lgd,\n",
    "['$p(y = t\\,|\\, x, \\ w)$', '$\\mathcal{H}(y\\,|\\, x, w)$'],\n",
    "                prop={'size': 25, 'weight': 'normal'})\n",
    "\n",
    "\n",
    "plt.autoscale(enable=True, axis='x', tight=True)\n",
    "\n",
    "Nim = 82\n",
    "s_rot = 0\n",
    "end_rot = 179\n",
    "steps = 10\n",
    "rotations = (np.linspace(s_rot, end_rot, steps)).astype(int)\n",
    "\n",
    "for i in range(1,len(rotations)):\n",
    "    angle = rotations[i]\n",
    "    # x_rot = ndim.interpolation.rotate(x_dev[Nim, :, :, :], angle, axes = (1,2), reshape=False)\n",
    "    x_rot = ndim.interpolation.rotate(valset.data[Nim], angle, axes = (0,1), reshape=False, mode='nearest')\n",
    "    ax3 = fig.add_subplot(3, (steps-1), 2*(steps-1)+i)\n",
    "    # ax3.imshow(np.transpose(x_rot,(1,2,0)))\n",
    "    ax3.imshow(x_rot)\n",
    "    ax3.axis('off')\n",
    "    ax3.set_xticklabels([])\n",
    "    ax3.set_yticklabels([])\n",
    "\n",
    "    \n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] + [ax2.title, ax2.xaxis.label, ax2.yaxis.label] +\n",
    "            ax.get_xticklabels() + ax.get_yticklabels() + ax2.get_xticklabels() + ax2.get_yticklabels()):\n",
    "    item.set_fontsize(fs)\n",
    "    item.set_weight('normal')\n",
    "\n",
    "plt.autoscale(enable=True, axis='x', tight=True)\n",
    "ax.set_ylim(bottom=0, top=1)\n",
    "ax2.set_ylim(bottom=0, top=2)\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check shapes of files\n",
    "\n",
    "dirname = 'Results/BBP_results/bbb'\n",
    "correct_preds = np.load(dirname + '/correct_preds.npy')\n",
    "all_preds = np.load(dirname + '/all_preds.npy')\n",
    "all_sample_preds = np.load(dirname + '/all_sample_preds.npy')\n",
    "\n",
    "print(correct_preds.shape, all_preds.shape, all_sample_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_name_list = ['Results/Regular_results', 'Results/MCdrop_results', 'Results/Ensemble_results']\n",
    "names = ['Regular NN', 'MC Dropout', 'Boostrap Ensemble']\n",
    "\n",
    "s_rot = 0\n",
    "end_rot = 179\n",
    "steps = 16\n",
    "rotations = (np.linspace(s_rot, end_rot, steps)).astype(int)\n",
    "\n",
    "# plt.figure(dpi=120)\n",
    "\n",
    "fs = 15\n",
    "Ncols = len(result_name_list)\n",
    "\n",
    "f, ax_vec = plt.subplots(1, Ncols, figsize=(6.5*Ncols, 5)) # , sharey=True\n",
    "\n",
    "\n",
    "for idx, dirname in enumerate(result_name_list):\n",
    "    \n",
    "    ax = ax_vec[idx]\n",
    "    ax2 = ax.twinx()\n",
    "\n",
    "    correct_preds = np.load(dirname + '/correct_preds.npy')\n",
    "    all_preds = np.load(dirname + '/all_preds.npy')\n",
    "    all_sample_preds = np.load(dirname + '/all_sample_preds.npy')\n",
    "    all_sample_preds = all_sample_preds + 1e-5\n",
    "\n",
    "    # print(correct_preds.shape)\n",
    "    # print(all_sample_preds.shape)\n",
    "\n",
    "    # Get correct prediction probabilities\n",
    "    correct_mean = correct_preds.mean(axis=0)\n",
    "    correct_std = correct_preds.std(axis=0)\n",
    "\n",
    "    # Compute the approx posterior predictive\n",
    "    posterior_preds = all_sample_preds.mean(axis=1)\n",
    "    total_entropy = -(posterior_preds * np.log(posterior_preds)).sum(axis=2)\n",
    "    total_entropy_mean = total_entropy.mean(axis=0)\n",
    "    total_entropy_std = total_entropy.std(axis=0)\n",
    "\n",
    "    # Get sample wise metrics (entropy) -> aleatoric entropy\n",
    "    sample_preds_entropy = -( all_sample_preds * np.log(all_sample_preds) ).sum(axis=3)\n",
    "    aleatoric_entropy = sample_preds_entropy.mean(axis=1)\n",
    "    aleatoric_entropy_mean = aleatoric_entropy.mean(axis=0)\n",
    "    aleatoric_entropy_std = aleatoric_entropy.std(axis=0)\n",
    "\n",
    "    # Get epistemic entropy \n",
    "    epistemic_entropy = total_entropy - aleatoric_entropy\n",
    "    epistemic_entropy_mean = epistemic_entropy.mean(axis=0)\n",
    "    epistemic_entropy_std = epistemic_entropy.std(axis=0)\n",
    "\n",
    "    # print('expected entropy mean', aleatoric_entropy_mean.shape)\n",
    "    # print('posterior entropy mean', posterior_mean_angle_entropy.shape)\n",
    "\n",
    "\n",
    "    aa, = errorfill(rotations, correct_mean, yerr=correct_std, color=c[2], alpha_fill=0.25, ax=ax)\n",
    "\n",
    "#     ax = plt.gca()\n",
    "    ax.set_xlabel('rotation angle')\n",
    "    ax.set_ylabel('predictive accuracy')\n",
    "    ax.set_title('%s' % names[idx])\n",
    "    ax.yaxis.grid() \n",
    "    ax.xaxis.grid()\n",
    "\n",
    "    \n",
    "    ax2.set_ylabel('nats')\n",
    "    bb, = errorfill(rotations, total_entropy_mean, yerr=total_entropy_std, color=c[0], ax=ax2)\n",
    "    cc, = errorfill(rotations, aleatoric_entropy_mean, yerr=aleatoric_entropy_std, color=c[1], ax=ax2)\n",
    "    dd, = errorfill(rotations, epistemic_entropy_mean, yerr=epistemic_entropy_std, color=c[3], ax=ax2)\n",
    "\n",
    "\n",
    "    # lgd = plt.legend(['correct class', 'posterior predictive entropy'], loc='upper right',\n",
    "    #                  prop={'size': 15, 'weight': 'normal'}, bbox_to_anchor=(1.4,1))\n",
    "\n",
    "\n",
    "    for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] + [ax2.title, ax2.xaxis.label, ax2.yaxis.label] +\n",
    "                ax.get_xticklabels() + ax.get_yticklabels() + ax2.get_xticklabels() + ax2.get_yticklabels()):\n",
    "        item.set_fontsize(fs)\n",
    "        item.set_weight('normal')\n",
    "\n",
    "    plt.autoscale(enable=True, axis='x', tight=True)\n",
    "    ax.set_ylim(bottom=0, top=1)\n",
    "    ax2.set_ylim(bottom=0, top=2)\n",
    "\n",
    "p_lgd = [aa, bb, cc, dd]\n",
    "lgd = plt.legend(p_lgd, ['$p(Y = t\\,|\\,x, w)$', '$\\mathcal{H}(y\\,|\\,x)$', '$E_{q(w)}[\\mathcal{H}(y\\,|\\,x, w)]$', '$\\mathcal{H}(\\mathbb{E}) - \\mathbb{E}[\\mathcal{H}]$'], bbox_to_anchor=(1.7,1))\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_name_list = ['Results/BBP_results/bbb', 'Results/BBP_results/lrt', 'Results/Contrastive_Reasoning_results', 'Results/Tent_results']\n",
    "names = ['BBP:Without lrt' , 'BBP: With lrt', 'Contrastive Reasoning', 'TENT']\n",
    "\n",
    "s_rot = 0\n",
    "end_rot = 179\n",
    "steps = 16\n",
    "rotations = (np.linspace(s_rot, end_rot, steps)).astype(int)\n",
    "\n",
    "# plt.figure(dpi=120)\n",
    "\n",
    "fs = 15\n",
    "Ncols = len(result_name_list)\n",
    "\n",
    "f, ax_vec = plt.subplots(1, Ncols, figsize=(6.5*Ncols, 5)) # , sharey=True\n",
    "\n",
    "\n",
    "for idx, dirname in enumerate(result_name_list):\n",
    "    \n",
    "    ax = ax_vec[idx]\n",
    "    ax2 = ax.twinx()\n",
    "\n",
    "    correct_preds = np.load(dirname + '/correct_preds.npy')\n",
    "    all_preds = np.load(dirname + '/all_preds.npy')\n",
    "    all_sample_preds = np.load(dirname + '/all_sample_preds.npy')\n",
    "    all_sample_preds = all_sample_preds + 1e-5\n",
    "\n",
    "    # print(correct_preds.shape)\n",
    "    # print(all_sample_preds.shape)\n",
    "\n",
    "    # Get correct prediction probabilities\n",
    "    correct_mean = correct_preds.mean(axis=0)\n",
    "    correct_std = correct_preds.std(axis=0)\n",
    "\n",
    "    # Compute the approx posterior predictive\n",
    "    posterior_preds = all_sample_preds.mean(axis=1)\n",
    "    total_entropy = -(posterior_preds * np.log(posterior_preds)).sum(axis=2)\n",
    "    total_entropy_mean = total_entropy.mean(axis=0)\n",
    "    total_entropy_std = total_entropy.std(axis=0)\n",
    "\n",
    "    # Get sample wise metrics (entropy) -> aleatoric entropy\n",
    "    sample_preds_entropy = -( all_sample_preds * np.log(all_sample_preds) ).sum(axis=3)\n",
    "    aleatoric_entropy = sample_preds_entropy.mean(axis=1)\n",
    "    aleatoric_entropy_mean = aleatoric_entropy.mean(axis=0)\n",
    "    aleatoric_entropy_std = aleatoric_entropy.std(axis=0)\n",
    "\n",
    "    # Get epistemic entropy \n",
    "    epistemic_entropy = total_entropy - aleatoric_entropy\n",
    "    epistemic_entropy_mean = epistemic_entropy.mean(axis=0)\n",
    "    epistemic_entropy_std = epistemic_entropy.std(axis=0)\n",
    "\n",
    "    # print('expected entropy mean', aleatoric_entropy_mean.shape)\n",
    "    # print('posterior entropy mean', posterior_mean_angle_entropy.shape)\n",
    "\n",
    "\n",
    "    aa, = errorfill(rotations, correct_mean, yerr=correct_std, color=c[2], alpha_fill=0.25, ax=ax)\n",
    "\n",
    "#     ax = plt.gca()\n",
    "    ax.set_xlabel('rotation angle')\n",
    "    ax.set_ylabel('predictive accuracy')\n",
    "    ax.set_title('%s' % names[idx])\n",
    "    ax.yaxis.grid() \n",
    "    ax.xaxis.grid()\n",
    "\n",
    "    \n",
    "    ax2.set_ylabel('nats')\n",
    "    bb, = errorfill(rotations, total_entropy_mean, yerr=total_entropy_std, color=c[0], ax=ax2)\n",
    "    cc, = errorfill(rotations, aleatoric_entropy_mean, yerr=aleatoric_entropy_std, color=c[1], ax=ax2)\n",
    "    dd, = errorfill(rotations, epistemic_entropy_mean, yerr=epistemic_entropy_std, color=c[3], ax=ax2)\n",
    "\n",
    "\n",
    "    # lgd = plt.legend(['correct class', 'posterior predictive entropy'], loc='upper right',\n",
    "    #                  prop={'size': 15, 'weight': 'normal'}, bbox_to_anchor=(1.4,1))\n",
    "\n",
    "\n",
    "    for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] + [ax2.title, ax2.xaxis.label, ax2.yaxis.label] +\n",
    "                ax.get_xticklabels() + ax.get_yticklabels() + ax2.get_xticklabels() + ax2.get_yticklabels()):\n",
    "        item.set_fontsize(fs)\n",
    "        item.set_weight('normal')\n",
    "\n",
    "    plt.autoscale(enable=True, axis='x', tight=True)\n",
    "    ax.set_ylim(bottom=0, top=1)\n",
    "    ax2.set_ylim(bottom=0, top=2)\n",
    "\n",
    "    \n",
    "p_lgd = [aa, bb, cc, dd]\n",
    "lgd = plt.legend(p_lgd, ['$p(Y = t\\,|\\,x, w)$', '$\\mathcal{H}(y\\,|\\,x)$', '$E_{q(w)}[\\mathcal{H}(y\\,|\\,x, w)]$', '$\\mathcal{H}(\\mathbb{E}) - \\mathbb{E}[\\mathcal{H}]$'], bbox_to_anchor=(1.7,1))\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictive_entropy(correct_preds, all_preds, rotations):\n",
    "    all_preds_entropy = -(all_preds * np.log(all_preds)).sum(axis=2)\n",
    "    mean_angle_entropy = all_preds_entropy.mean(axis=0)\n",
    "    std_angle_entropy = all_preds_entropy.std(axis=0)\n",
    "\n",
    "    correct_mean = correct_preds.mean(axis=0)\n",
    "    correct_std = correct_preds.std(axis=0)\n",
    "\n",
    "    # plt.figure(dpi=100)\n",
    "    line_ax0 = errorfill(rotations, correct_mean, yerr=correct_std, color=c[2])\n",
    "    ax = plt.gca()\n",
    "    ax2 = ax.twinx()\n",
    "    line_ax1 = errorfill(rotations, mean_angle_entropy, yerr=std_angle_entropy, color=c[3], ax=ax2)\n",
    "    plt.xlabel('rotation angle')\n",
    "    lns = line_ax0+line_ax1\n",
    "\n",
    "    # lgd = plt.legend(lns, ['correct class', 'predictive entropy'], loc='upper right',\n",
    "    #                 prop={'size': 15, 'weight': 'normal'}, bbox_to_anchor=(1.75,1))\n",
    "\n",
    "\n",
    "    for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] + [ax2.title, ax2.xaxis.label, ax2.yaxis.label] +\n",
    "                ax.get_xticklabels() + ax.get_yticklabels() + ax2.get_xticklabels() + ax2.get_yticklabels()):\n",
    "        item.set_fontsize(12)\n",
    "        item.set_weight('normal')\n",
    "    plt.autoscale(enable=True, axis='x', tight=True)\n",
    "\n",
    "result_name_list = ['Results/Regular_results', 'Results/MCdrop_results', 'Results/Ensemble_results',\n",
    "                     'Results/BBP_results/bbb', 'Results/BBP_results/lrt', 'Results/GradCon_results']\n",
    "names = [\"Regular\", 'MC Dropout', 'Bootstrap Ensemble', 'BBP: without lrt' , 'BBP: with lrt', 'GradCon']\n",
    "\n",
    "steps = 16\n",
    "rotations = (np.linspace(0, 179, steps)).astype(int)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "for n,dir in enumerate(result_name_list):\n",
    "    correct_preds = np.load(dir + '/correct_preds.npy')\n",
    "    all_preds = np.load(dir + '/all_preds.npy')\n",
    "    \n",
    "    plt.subplot(2,3,n+1)\n",
    "    plt.title(names[n])\n",
    "    plot_predictive_entropy(correct_preds, all_preds, rotations)\n",
    "\n",
    "# lgd = plt.legend(['correct class', 'predictive entropy'], loc='upper right',\n",
    "#                     prop={'size': 15, 'weight': 'normal'}, bbox_to_anchor=(1.75,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = 'Results/MCdrop_results'\n",
    "\n",
    "correct_preds = np.load(dirname + '/correct_preds.npy')\n",
    "# all_preds = np.load(dirname + '/all_preds.npy')\n",
    "all_sample_preds = np.load(dirname + '/all_sample_preds.npy')\n",
    "all_sample_preds = all_sample_preds + 1e-5\n",
    "\n",
    "# print(correct_preds.shape)\n",
    "# print(all_sample_preds.shape)\n",
    "\n",
    "# Get correct prediction probabilities\n",
    "correct_mean = correct_preds.mean(axis=0)\n",
    "correct_std = correct_preds.std(axis=0)\n",
    "# print(correct_preds.shape)\n",
    "\n",
    "print(all_sample_preds.shape)\n",
    "# Total entropy\n",
    "posterior_preds = all_sample_preds.mean(axis=1) # Mean across 100 samples\n",
    "total_entropy = -(posterior_preds * np.log(posterior_preds)).sum(axis=2) # Entropy\n",
    "total_entropy_mean = total_entropy.mean(axis=0) \n",
    "total_entropy_std = total_entropy.std(axis=0)\n",
    "print(total_entropy.shape)\n",
    "\n",
    "# Aleatoric entropy\n",
    "sample_preds_entropy = -( all_sample_preds * np.log(all_sample_preds) ).sum(axis=3) # Entropy\n",
    "aleatoric_entropy = sample_preds_entropy.mean(axis=1) # Mean across 100 samples\n",
    "aleatoric_entropy_mean = aleatoric_entropy.mean(axis=0)\n",
    "aleatoric_entropy_std = aleatoric_entropy.std(axis=0)\n",
    "print(aleatoric_entropy.shape)\n",
    "\n",
    "# Epistemic entropy \n",
    "epistemic_entropy = total_entropy - aleatoric_entropy # Difference\n",
    "epistemic_entropy_mean = epistemic_entropy.mean(axis=0)\n",
    "epistemic_entropy_std = epistemic_entropy.std(axis=0)\n",
    "print(epistemic_entropy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot(targets, nb_classes):\n",
    "    res = np.eye(nb_classes)[np.array(targets).reshape(-1)]\n",
    "    return res.reshape(list(targets.shape)+[nb_classes])\n",
    "\n",
    "result_name_list = ['Results/Regular_results', 'Results/MCdrop_results', 'Results/Ensemble_results',\n",
    "                     'Results/BBP_results/bbb', 'Results/BBP_results/lrt', 'Results/Contrastive_Reasoning_results', 'Results/Tent_results']\n",
    "names = [\"Regular\", 'MC Dropout', 'Bootstrap Ensemble', 'BBP: without lrt' , 'BBP: with lrt', 'Contrastive Reasoning', 'TENT']\n",
    "\n",
    "bin_step = 0.1\n",
    "bins = np.arange(0, 1.0001, bin_step)\n",
    "\n",
    "fig = plt.figure(dpi=120)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "min_idx = 10\n",
    "\n",
    "for dir_idx, dirname in enumerate(result_name_list):\n",
    "    \n",
    "\n",
    "    targets = y_dev\n",
    "    all_preds = np.load(dirname + '/all_preds.npy')\n",
    "\n",
    "\n",
    "    all_preds = all_preds[:, 0, :]\n",
    "    pred_class = np.argmax(all_preds, axis=1)\n",
    "    # targets = targets[:,0]\n",
    "    # targets = np.around(targets).astype(int)\n",
    "\n",
    "    expanded_preds = np.reshape(all_preds, -1)\n",
    "    pred_class_OH_expand = np.reshape(get_one_hot(pred_class, 10), -1)\n",
    "    targets_class_OH_expand = np.reshape(get_one_hot(targets, 10), -1)\n",
    "    \n",
    "    correct_vec = (targets_class_OH_expand*(pred_class_OH_expand == targets_class_OH_expand)).astype(int)\n",
    "#     correct_vec = (pred_class == targets).astype(int)\n",
    "\n",
    "    bin_idxs = np.digitize(expanded_preds, bins, right=True)\n",
    "    bin_idxs = bin_idxs - 1\n",
    "\n",
    "    bin_centers = bins[1:] - bin_step/2\n",
    "\n",
    "    bin_counts = np.ones(len(bin_centers))\n",
    "    bin_corrects = np.zeros(len(bin_centers))\n",
    "    \n",
    "#     print(min(bin_idxs))\n",
    "    if min(bin_idxs) < min_idx:\n",
    "        min_idx = min(bin_idxs)\n",
    "\n",
    "    for nbin in range(len(bin_centers)):\n",
    "\n",
    "        bin_counts[nbin] = np.sum((bin_idxs==nbin).astype(int))\n",
    "        bin_corrects[nbin] = np.sum(correct_vec[bin_idxs==nbin])\n",
    "\n",
    "    have_data = bin_counts > 0  \n",
    "    bin_pcorr = bin_corrects[have_data] / bin_counts[have_data]\n",
    "\n",
    "    ax.plot(bin_centers[have_data], bin_pcorr, '-o', label=names[dir_idx])\n",
    "\n",
    "# print(min_idx)\n",
    "\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "ax.plot(bin_centers[min_idx:], bin_centers[min_idx:], '--', c='k')\n",
    "ax.set_xticks(bin_centers[min_idx:]) # [labels]\n",
    "\n",
    "ax.set_xlabel('Predicted probability')\n",
    "ax.set_ylabel('Correct proportion')\n",
    "ax.set_title('Calibration Curve')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10 Rotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot(targets, nb_classes):\n",
    "    res = np.eye(nb_classes)[np.array(targets).reshape(-1)]\n",
    "    return res.reshape(list(targets.shape)+[nb_classes])\n",
    "\n",
    "result_name_list = ['Results/Regular_results', 'Results/MCdrop_results', 'Results/Ensemble_results',\n",
    "                     'Results/BBP_results/bbb', 'Results/BBP_results/lrt', 'Results/Contrastive_Reasoning_results', 'Results/Tent_results']\n",
    "names = [\"Regular\", 'MC Dropout', 'Bootstrap Ensemble', 'BBP: without lrt' , 'BBP: with lrt', 'Contrastive Reasoning', 'TENT']\n",
    "bin_step = 0.1\n",
    "bins = np.arange(0, 1.0001, bin_step)\n",
    "\n",
    "fig = plt.figure(dpi=120)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "min_idx = 10\n",
    "targets = np.repeat(y_dev,16)\n",
    "\n",
    "for dir_idx, dirname in enumerate(result_name_list):\n",
    "    \n",
    "    all_preds = np.load(dirname + '/all_preds.npy')\n",
    "    N, R, C = all_preds.shape\n",
    "    all_preds = all_preds.reshape(-1, C) # Over all rotations\n",
    "    pred_class = np.argmax(all_preds, axis=1)\n",
    "    # targets = targets[:,0]\n",
    "    # targets = np.around(targets).astype(int)\n",
    "\n",
    "    expanded_preds = np.reshape(all_preds, -1)\n",
    "    pred_class_OH_expand = np.reshape(get_one_hot(pred_class, 10), -1)\n",
    "    targets_class_OH_expand = np.reshape(get_one_hot(targets, 10), -1)\n",
    "    \n",
    "    correct_vec = (targets_class_OH_expand*(pred_class_OH_expand == targets_class_OH_expand)).astype(int)\n",
    "#     correct_vec = (pred_class == targets).astype(int)\n",
    "\n",
    "    bin_idxs = np.digitize(expanded_preds, bins, right=True)\n",
    "    bin_idxs = bin_idxs - 1\n",
    "\n",
    "    bin_centers = bins[1:] - bin_step/2\n",
    "\n",
    "    bin_counts = np.ones(len(bin_centers))\n",
    "    bin_corrects = np.zeros(len(bin_centers))\n",
    "    \n",
    "#     print(min(bin_idxs))\n",
    "    if min(bin_idxs) < min_idx:\n",
    "        min_idx = min(bin_idxs)\n",
    "\n",
    "    for nbin in range(len(bin_centers)):\n",
    "\n",
    "        bin_counts[nbin] = np.sum((bin_idxs==nbin).astype(int))\n",
    "        bin_corrects[nbin] = np.sum(correct_vec[bin_idxs==nbin])\n",
    "\n",
    "    have_data = bin_counts > 0  \n",
    "    bin_pcorr = bin_corrects[have_data] / bin_counts[have_data]\n",
    "\n",
    "    ax.plot(bin_centers[have_data], bin_pcorr, '-o', label=names[dir_idx])\n",
    "\n",
    "# print(min_idx)\n",
    "\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "ax.plot(bin_centers[min_idx:], bin_centers[min_idx:], '--', c='k')\n",
    "ax.set_xticks(bin_centers[min_idx:]) # [labels]\n",
    "\n",
    "ax.set_xlabel('Predicted probability')\n",
    "ax.set_ylabel('Correct proportion')\n",
    "ax.set_title('Calibration Curve')\n",
    "plt.savefig('cc_cifar10rot.pdf', bbox_extra_artists=(lgd,), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10-C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot(targets, nb_classes):\n",
    "    res = np.eye(nb_classes)[np.array(targets).reshape(-1)]\n",
    "    return res.reshape(list(targets.shape)+[nb_classes])\n",
    "\n",
    "result_name_list = ['Results/Regular_results', 'Results/MCdrop_results', 'Results/Ensemble_results',\n",
    "                     'Results/BBP_results/bbb', 'Results/BBP_results/lrt', 'Results/Contrastive_Reasoning_results', 'Results/Tent_results']\n",
    "names = [\"Regular\", 'MC Dropout', 'Bootstrap Ensemble', 'BBP: without lrt' , 'BBP: with lrt', 'Contrastive Reasoning', 'TENT']\n",
    "\n",
    "bin_step = 0.1\n",
    "bins = np.arange(0, 1.0001, bin_step)\n",
    "\n",
    "fig = plt.figure(dpi=120)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "min_idx = 10\n",
    "targets = np.tile(y_dev,5*19)\n",
    "\n",
    "for dir_idx, dirname in enumerate(result_name_list):\n",
    "    \n",
    "\n",
    "    all_preds = np.load(dirname + '/preds_CIFAR-10-C.npy')\n",
    "\n",
    "\n",
    "    # all_preds = all_preds[:, 0, :]\n",
    "    pred_class = np.argmax(all_preds, axis=1)\n",
    "    # targets = targets[:,0]\n",
    "    # targets = np.around(targets).astype(int)\n",
    "\n",
    "    expanded_preds = np.reshape(all_preds, -1)\n",
    "    pred_class_OH_expand = np.reshape(get_one_hot(pred_class, 10), -1)\n",
    "    targets_class_OH_expand = np.reshape(get_one_hot(targets, 10), -1)\n",
    "    \n",
    "    correct_vec = (targets_class_OH_expand*(pred_class_OH_expand == targets_class_OH_expand)).astype(int)\n",
    "#     correct_vec = (pred_class == targets).astype(int)\n",
    "\n",
    "    bin_idxs = np.digitize(expanded_preds, bins, right=True)\n",
    "    bin_idxs = bin_idxs - 1\n",
    "\n",
    "    bin_centers = bins[1:] - bin_step/2\n",
    "\n",
    "    bin_counts = np.ones(len(bin_centers))\n",
    "    bin_corrects = np.zeros(len(bin_centers))\n",
    "    \n",
    "#     print(min(bin_idxs))\n",
    "    if min(bin_idxs) < min_idx:\n",
    "        min_idx = min(bin_idxs)\n",
    "\n",
    "    for nbin in range(len(bin_centers)):\n",
    "\n",
    "        bin_counts[nbin] = np.sum((bin_idxs==nbin).astype(int))\n",
    "        bin_corrects[nbin] = np.sum(correct_vec[bin_idxs==nbin])\n",
    "\n",
    "    have_data = bin_counts > 0  \n",
    "    bin_pcorr = bin_corrects[have_data] / bin_counts[have_data]\n",
    "\n",
    "    ax.plot(bin_centers[have_data], bin_pcorr, '-o', label=names[dir_idx])\n",
    "\n",
    "# print(min_idx)\n",
    "\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "ax.plot(bin_centers[min_idx:], bin_centers[min_idx:], '--', c='k')\n",
    "ax.set_xticks(bin_centers[min_idx:]) # [labels]\n",
    "\n",
    "ax.set_xlabel('Predicted probability')\n",
    "ax.set_ylabel('Correct proportion')\n",
    "ax.set_title('Calibration Curve')\n",
    "plt.savefig('cc_cifar10c.pdf', bbox_extra_artists=(lgd,), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eed44ad03d3398d64d3d4bda079817b6c24e225a548088d53b43c511791e0bd9"
  },
  "kernelspec": {
   "display_name": "Python 3.6.8 ('megabayes')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
