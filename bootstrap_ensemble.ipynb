{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms, datasets\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "from networks import Bootstrap_Net\n",
    "from utils import *\n",
    "from utils_plotting import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation\n",
    "batch_size = 128\n",
    "mean = (0.4914, 0.4822, 0.4465)\n",
    "std = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "transform_train = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.RandomCrop(32, padding=4),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean, std)\n",
    "                                    ])\n",
    "transform_test = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean, std)])\n",
    "\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "valset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "models_dir = 'Models/Ensemble_models'\n",
    "results_dir = 'Results/Ensemble_results'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap part\n",
    "Nruns = 25 # Number of nets in ensemble.\n",
    "num_epochs = 200 # Epochs per net\n",
    "p_subsample = 0.8 # Rate at which to subsample the dataset to train each net in the ensemble\n",
    "weight_set_samples = [] # Save model state dicts\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for iii in range(Nruns):\n",
    "    keep_idx = []\n",
    "    \n",
    "    for idx in range(len(trainset)):\n",
    "        if np.random.binomial(1, p_subsample, size=1) == 1:\n",
    "            keep_idx.append(idx)\n",
    "\n",
    "    keep_idx = np.array(keep_idx)\n",
    "\n",
    "    from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "    sampler = SubsetRandomSampler(keep_idx)\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=False, num_workers=4, sampler=sampler)\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False,num_workers=4)\n",
    "\n",
    "    ###############################################################\n",
    "    # net = Bootstrap_Net(lr=lr, channels_in=3, side_in=32, cuda=use_cuda, classes=10, batch_size=batch_size,weight_decay=weight_decay)\n",
    "    net = Bootstrap_Net(18,10)\n",
    "    net.cuda()\n",
    "\n",
    "    best_acc = 0\n",
    "    start = time.time()\n",
    "    elapsed_time = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Train\n",
    "        net.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        lr = 0.1\n",
    "        optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate(lr, epoch), momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "        # print('\\n=> Training Epoch #%d, LR=%.4f' %(epoch, learning_rate(lr, epoch)))\n",
    "        for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda() # GPU settings\n",
    "            optimizer.zero_grad()\n",
    "            inputs, targets = Variable(inputs), Variable(targets)\n",
    "            outputs = net(inputs)               # Forward Propagation\n",
    "            loss = criterion(outputs, targets)  # Loss\n",
    "            loss.backward()  # Backward Propagation\n",
    "            optimizer.step() # Optimizer update\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "            # sys.stdout.flush()\n",
    "        # print('| Epoch [%3d/%3d] Iter[%3d/%3d]\\t\\tLoss: %.4f Acc@1: %.3f%%'\n",
    "        #             %(epoch, num_epochs, batch_idx+1,\n",
    "        #                 (len(trainset)//batch_size)+1, loss.item(), 100.*correct/total))\n",
    "        \n",
    "        # Test\n",
    "        net.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for batch_idx, (inputs, targets) in enumerate(valloader):\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            with torch.no_grad():\n",
    "                inputs, targets = Variable(inputs), Variable(targets)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "        # Save checkpoint when best model\n",
    "        acc = 100.*correct/total\n",
    "        # print(\"\\n| Validation Epoch #%d\\t\\t\\tLoss: %.4f Acc@1: %.2f%%\" %(epoch, loss.item(), acc))\n",
    "\n",
    "        if acc > best_acc:\n",
    "            # print('| Saving Best model...\\t\\t\\tTop1 = %.2f%%' %(acc))\n",
    "            # state = {\n",
    "            #         'net':net.state_dict if use_cuda else net,\n",
    "            #         'acc':acc,\n",
    "            #         'epoch':epoch,\n",
    "            # }\n",
    "\n",
    "            # torch.save(state, models_dir + '/' + 'theta_best.t7')\n",
    "            best_acc = acc\n",
    "\n",
    "        epoch_time = time.time() - start_time\n",
    "        elapsed_time += epoch_time\n",
    "        # print('| Elapsed time : %d:%02d:%02d'  %(get_hms(elapsed_time)))\n",
    "\n",
    "    print('\\n Net %d: Testing model'%(iii))\n",
    "    print('* Test results : Acc@1 = %.2f%%' %(best_acc))\n",
    "    end = time.time()\n",
    "    print('| Total time : %d:%02d:%02d'  %(get_hms(end - start)))\n",
    "\n",
    "    weight_set_samples.append(copy.deepcopy(net.state_dict()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object(weight_set_samples, models_dir+'/state_dicts.pkl') # save all weight configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = Bootstrap_Net(lr=lr, channels_in=3, side_in=32, cuda=use_cuda, classes=10, batch_size=batch_size,weight_decay=weight_decay)\n",
    "net = Bootstrap_Net(18,10)\n",
    "net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Test\n",
    "net.eval()\n",
    "\n",
    "wrong = 0\n",
    "for batch_idx, (inputs, targets) in enumerate(valloader):\n",
    "    if use_cuda:\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "    with torch.no_grad():\n",
    "        inputs, targets = Variable(inputs), Variable(targets)\n",
    "    cost, err, probs = net.sample_eval(inputs, targets, weight_set_samples, logits=False)\n",
    "    # loss = criterion(outputs, targets)\n",
    "    wrong += err.item()\n",
    "    \n",
    "acc = 1-(wrong/len(valset))\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_set_samples = load_object(models_dir+'/state_dicts.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10 Rotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotations\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=True,\n",
    "                                                num_workers=3)\n",
    "x_dev = []\n",
    "y_dev = []\n",
    "for x, y in valloader:\n",
    "    x_dev.append(x.cpu().numpy())\n",
    "    y_dev.append(y.cpu().numpy())\n",
    "\n",
    "x_dev = np.concatenate(x_dev)\n",
    "y_dev = np.concatenate(y_dev)\n",
    "print(x_dev.shape)\n",
    "print(y_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nsamples = len(weight_set_samples)\n",
    "im_ind = 23\n",
    "steps = 16\n",
    "im_list = valset.test_data\n",
    "\n",
    "plt.figure()\n",
    "# plt.imshow( ndim.interpolation.rotate(np.transpose(x_dev[im_ind,:,:,:],(1,2,0)), 0, reshape=False))\n",
    "plt.imshow(im_list[im_ind])\n",
    "plt.title('original image')\n",
    "plt.savefig(results_dir + '/sample_image.png', bbox_inches='tight')\n",
    "s_rot = 0\n",
    "end_rot = 179\n",
    "rotations = (np.linspace(s_rot, end_rot, steps)).astype(int)            \n",
    "\n",
    "ims = []\n",
    "predictions = []\n",
    "# percentile_dist_confidence = []\n",
    "x, y = x_dev[im_ind], y_dev[im_ind]\n",
    "\n",
    "fig = plt.figure(figsize=(steps, 8), dpi=80)\n",
    "\n",
    "# DO ROTATIONS ON OUR IMAGE\n",
    "\n",
    "for i in range(len(rotations)):\n",
    "    \n",
    "    angle = rotations[i]\n",
    "    x_rot = ndim.interpolation.rotate(x, angle, axes=(1,2),reshape=False, mode='nearest')\n",
    "    \n",
    "    ax = fig.add_subplot(3, (steps-1), 2*(steps-1)+i)  \n",
    "    # ax.imshow(np.transpose(x_rot,(1,2,0))) # Image pixels lie in [-1,1]\n",
    "    ax.imshow(ndim.interpolation.rotate(im_list[im_ind],angle, axes=(0,1),reshape=False ,mode='nearest'))\n",
    "    ax.axis('off')\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ims.append(x_rot)\n",
    "    \n",
    "ims = np.array(ims)\n",
    "print(ims.shape)\n",
    "y = np.ones(ims.shape[0])*y\n",
    "# ims = np.expand_dims(ims, axis=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    cost, err, probs = net.sample_eval(torch.from_numpy(ims), torch.from_numpy(y), weight_set_samples, logits=False) # , logits=True\n",
    "\n",
    "predictions = probs.numpy()    \n",
    "textsize = 15\n",
    "lw = 5\n",
    "\n",
    "c = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd',\n",
    "    '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']  \n",
    "\n",
    "ax0 = plt.subplot2grid((3, steps-1), (0, 0), rowspan=2, colspan=steps-1)\n",
    "#ax0 = fig.add_subplot(2, 1, 1)\n",
    "plt.gca().set_prop_cycle(color = c)\n",
    "ax0.plot(rotations, predictions, linewidth=lw)\n",
    "\n",
    "\n",
    "##########################\n",
    "# Dots at max\n",
    "\n",
    "for i in range(predictions.shape[1]):\n",
    "\n",
    "    selections = (predictions[:,i] == predictions.max(axis=1))\n",
    "    for n in range(len(selections)):\n",
    "        if selections[n]:\n",
    "            ax0.plot(rotations[n], predictions[n, i], 'o', c=c[i], markersize=15.0)\n",
    "##########################  \n",
    "\n",
    "lgd = ax0.legend(['airplane', 'automobile', 'bird',\n",
    "            'cat', 'deer', 'dog',\n",
    "            'frog', 'horse', 'ship',\n",
    "            'truck'], loc='upper right', prop={'size': textsize, 'weight': 'normal'}, bbox_to_anchor=(1.35,1))\n",
    "plt.xlabel('rotation angle')\n",
    "# plt.ylabel('probability')\n",
    "plt.title('True class: %d, Nsamples %d' % (y[0], Nsamples))\n",
    "# ax0.axis('tight')\n",
    "plt.tight_layout()\n",
    "plt.autoscale(enable=True, axis='x', tight=True)\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "for item in ([ax0.title, ax0.xaxis.label, ax0.yaxis.label] +\n",
    "            ax0.get_xticklabels() + ax0.get_yticklabels()):\n",
    "    item.set_fontsize(textsize)\n",
    "    item.set_weight('normal')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rotated = np.load(\"data/CIFAR10_rotated.npy\")\n",
    "data_rotated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nsamples = len(weight_set_samples)\n",
    "\n",
    "s_rot = 0\n",
    "end_rot = 179\n",
    "steps = 16\n",
    "rotations = (np.linspace(s_rot, end_rot, steps)).astype(int)            \n",
    "\n",
    "def preprocess_test(X):\n",
    "\n",
    "    N, H, W, C = X.shape\n",
    "    Y = torch.zeros(N, C, H, W)\n",
    "    mean = (0.4914, 0.4822, 0.4465)\n",
    "    std = (0.2023, 0.1994, 0.2010)  \n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean,std),\n",
    "    ])\n",
    "\n",
    "    for n in range(len(X)):\n",
    "        Y[n] =  transform_test(X[n])\n",
    "\n",
    "    return Y  \n",
    "\n",
    "all_preds = np.zeros((len(x_dev), steps, 10))\n",
    "all_sample_preds = np.zeros((len(x_dev), Nsamples, steps, 10))\n",
    "\n",
    "# DO ROTATIONS ON OUR IMAGE\n",
    "for im_ind in range(len(x_dev)):\n",
    "    \n",
    "    if(im_ind % 500 == 0):\n",
    "        print(im_ind)\n",
    "    \n",
    "    y = y_dev[im_ind]\n",
    "    y = np.ones(ims.shape[0])*y\n",
    "\n",
    "    ims = data_rotated[:,im_ind,:,:,:]\n",
    "    ims = preprocess_test(ims)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        sample_probs = net.all_sample_eval(ims, torch.from_numpy(y), weight_set_samples)\n",
    "    probs = sample_probs.mean(dim=0)\n",
    "    \n",
    "    all_sample_preds[im_ind, :, :, :] = sample_probs.cpu().numpy()\n",
    "    predictions = probs.cpu().numpy()\n",
    "    all_preds[im_ind, :, :] = predictions\n",
    "   \n",
    "correct_preds = np.zeros((len(x_dev), steps))\n",
    "for i in range(len(x_dev)):\n",
    "    correct_preds[i,:] = all_preds[i,:,y_dev[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(results_dir+'/correct_preds.npy', correct_preds)\n",
    "np.save(results_dir+'/all_preds.npy', all_preds)\n",
    "np.save(results_dir+'/all_sample_preds.npy', all_sample_preds) #all_sample_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotations = (np.linspace(0, 179, steps)).astype(int)\n",
    "N = 10000\n",
    "\n",
    "correct_preds = np.zeros((N, steps))\n",
    "for i in range(N):\n",
    "    correct_preds[i,:] = all_preds[i,:,y_dev[i]]   \n",
    "\n",
    "plot_predictive_entropy(correct_preds, all_preds, rotations, results_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10-C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_set_samples = load_object(models_dir+'/state_dicts.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = Bootstrap_Net(lr=lr, channels_in=3, side_in=32, cuda=use_cuda, classes=10, batch_size=batch_size,weight_decay=weight_decay)\n",
    "net = Bootstrap_Net(18,10)\n",
    "net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chalPath = 'data/CIFAR-10-C/'\n",
    "chals = sorted(os.listdir(chalPath))\n",
    "\n",
    "chal_labels = valset.test_labels\n",
    "chal_labels = torch.Tensor(chal_labels)\n",
    "chal_labels = chal_labels.long()\n",
    "\n",
    "def preprocess_test(X):\n",
    "\n",
    "    N, H, W, C = X.shape\n",
    "    Y = torch.zeros(N, C, H, W)\n",
    "    mean = (0.4914, 0.4822, 0.4465)\n",
    "    std = (0.2023, 0.1994, 0.2010)  \n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean,std),\n",
    "    ])\n",
    "\n",
    "    for n in range(len(X)):\n",
    "        Y[n] =  transform_test(X[n])\n",
    "\n",
    "    return Y\n",
    "\n",
    "preds_list= []\n",
    "net.eval()\n",
    "avg_list = []\n",
    "\n",
    "for challenge in range(len(chals)):\n",
    "    chal_data = np.load(chalPath + chals[challenge])\n",
    "    # chal_data = np.transpose(chal_data, (0,3,1,2))\n",
    "\n",
    "    avg = 0\n",
    "    for j in range(5):\n",
    "        chal_temp_data = chal_data[j * 10000:(j + 1) * 10000]\n",
    "        chal_temp_data = preprocess_test(chal_temp_data)\n",
    "\n",
    "        chal_dataset = torch.utils.data.TensorDataset(chal_temp_data, chal_labels)\n",
    "        chal_loader = torch.utils.data.DataLoader(chal_dataset, batch_size=100)\n",
    "        chal_error = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x, y in chal_loader:\n",
    "                cost, err, probs = net.sample_eval(x, y, weight_set_samples=weight_set_samples, logits=False)\n",
    "                preds_list.append(probs.cpu().numpy())\n",
    "                chal_error += err.cpu().numpy()\n",
    "                # print(err)\n",
    "\n",
    "        # print(chal_error)\n",
    "        chal_acc = 1 - (chal_error/len(chal_dataset))\n",
    "        avg += chal_acc\n",
    "        print(chal_acc)\n",
    "    \n",
    "    avg /= 5\n",
    "    avg_list.append(avg)\n",
    "    print(\"Average:\",avg,\" \", chals[challenge])\n",
    "\n",
    "print(\"Mean: \", np.mean(avg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_list = np.vstack(preds_list)\n",
    "np.save(results_dir+'/preds_CIFAR-10-C.npy', preds_list)\n",
    "np.save(results_dir+'/avg_list_CIFAR-10-C.npy', avg_list)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eed44ad03d3398d64d3d4bda079817b6c24e225a548088d53b43c511791e0bd9"
  },
  "kernelspec": {
   "display_name": "Python 3.6.8 ('megabayes')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
