{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms, datasets\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import time\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from scipy.special import softmax\n",
    "\n",
    "from networks import MCDropout\n",
    "from utils import *\n",
    "from utils_plotting import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "mean = (0.4914, 0.4822, 0.4465)\n",
    "std = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "transform_train = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.RandomCrop(32, padding=4),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean, std)\n",
    "                                    ])\n",
    "transform_test = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean, std)])\n",
    "\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "valset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "models_dir = \"Models/\" + 'MCdrop_models'\n",
    "results_dir = \"Results/\" + 'MCdrop_results'\n",
    "\n",
    "# savemodel_its = [10,20,30,40,50]\n",
    "# save_dicts = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = MC_drop_net(lr=lr, channels_in=3, side_in=32, cuda=use_cuda, classes=10, batch_size=batch_size, weight_decay=weight_decay)\n",
    "net = MCDropout(18, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cuda:\n",
    "    net.cuda()\n",
    "    net = torch.nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\n",
    "    cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(epoch): \n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    lr = 0.1\n",
    "    \n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate(lr, epoch), momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "    print('\\n=> Training Epoch #%d, LR=%.4f' %(epoch, learning_rate(lr, epoch)))\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda() # GPU settings\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = Variable(inputs), Variable(targets)\n",
    "        outputs = net(inputs)               # Forward Propagation\n",
    "        loss = criterion(outputs, targets)  # Loss\n",
    "        loss.backward()  # Backward Propagation\n",
    "        optimizer.step() # Optimizer update\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "        # sys.stdout.write('\\r')\n",
    "        # sys.stdout.write('| Epoch [%3d/%3d] Iter[%3d/%3d]\\t\\tLoss: %.4f Acc@1: %.3f%%'\n",
    "        #         %(epoch, num_epochs, batch_idx+1,\n",
    "        #             (len(trainset)//batch_size)+1, loss.item(), 100.*correct/total))\n",
    "        # sys.stdout.flush()\n",
    "    print('| Epoch [%3d/%3d] Iter[%3d/%3d]\\t\\tLoss: %.4f Acc@1: %.3f%%'\n",
    "                %(epoch, num_epochs, batch_idx+1,\n",
    "                    (len(trainset)//batch_size)+1, loss.item(), 100.*correct/total))\n",
    "\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(valloader):\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        with torch.no_grad():\n",
    "            inputs, targets = Variable(inputs), Variable(targets)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "    # Save checkpoint when best model\n",
    "    acc = 100.*correct/total\n",
    "    print(\"\\n| Validation Epoch #%d\\t\\t\\tLoss: %.4f Acc@1: %.2f%%\" %(epoch, loss.item(), acc))\n",
    "\n",
    "    if acc > best_acc:\n",
    "        print('| Saving Best model...\\t\\t\\tTop1 = %.2f%%' %(acc))\n",
    "        state = {\n",
    "                'net':net.module if use_cuda else net,\n",
    "                'acc':acc,\n",
    "                'epoch':epoch,\n",
    "        }\n",
    "\n",
    "        torch.save(state, models_dir + '/' + 'theta_best.t7')\n",
    "        best_acc = acc\n",
    "\n",
    "def get_hms(seconds):\n",
    "    m, s = divmod(seconds, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "\n",
    "    return h, m, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "best_acc = 0\n",
    "\n",
    "start = time.time()\n",
    "elapsed_time = 0\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "\n",
    "    epoch_time = time.time() - start_time\n",
    "    elapsed_time += epoch_time\n",
    "    print('| Elapsed time : %d:%02d:%02d'  %(get_hms(elapsed_time)))\n",
    "\n",
    "print('\\n[Phase 4] : Testing model')\n",
    "print('* Test results : Acc@1 = %.2f%%' %(best_acc))\n",
    "end = time.time()\n",
    "print('| Total time : %d:%02d:%02d'  %(get_hms(end - start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#91.30\n",
    "#89.51"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(models_dir + '/' + 'theta_best.t7')\n",
    "net = checkpoint['net']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  CIFAR-10 Rotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rotations, Bayesian\n",
    "# valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=4)\n",
    "x_dev = []\n",
    "y_dev = []\n",
    "for x, y in valloader:\n",
    "    x_dev.append(x.cpu().numpy())\n",
    "    y_dev.append(y.cpu().numpy())\n",
    "\n",
    "x_dev = np.concatenate(x_dev)\n",
    "y_dev = np.concatenate(y_dev)\n",
    "print(x_dev.shape)\n",
    "print(y_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rotate(x_dev, y_dev, net, results_dir, im_list = valset.test_data ,im_ind = 23, Nsamples = 100, steps=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rotated = np.load(\"data/CIFAR10_rotated.npy\")\n",
    "# data_rotated = np.transpose(data_rotated, (0,1,4,2,3))\n",
    "data_rotated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 16\n",
    "N = 10000\n",
    "Nsamples = 100\n",
    "y_dev = valset.test_labels\n",
    "\n",
    "def preprocess_test(X):\n",
    "\n",
    "    N, H, W, C = X.shape\n",
    "    Y = torch.zeros(N, C, H, W)\n",
    "    mean = (0.4914, 0.4822, 0.4465)\n",
    "    std = (0.2023, 0.1994, 0.2010)  \n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean,std),\n",
    "    ])\n",
    "\n",
    "    for n in range(len(X)):\n",
    "        Y[n] =  transform_test(X[n])\n",
    "\n",
    "    return Y\n",
    "\n",
    "all_preds = np.zeros((N, steps, 10))\n",
    "all_sample_preds = np.zeros((N, Nsamples, steps, 10))\n",
    "\n",
    "net.eval()\n",
    "for im_ind in range(N):\n",
    "    if(im_ind % 500 == 0):\n",
    "        print(im_ind)\n",
    "\n",
    "    y =  y_dev[im_ind]\n",
    "    \n",
    "    ims = data_rotated[:,im_ind,:,:,:]\n",
    "    ims = preprocess_test(ims)\n",
    "\n",
    "    y = np.ones(ims.shape[0])*y\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        sample_probs = net.all_sample_eval(ims, torch.from_numpy(y), Nsamples=Nsamples)\n",
    "    probs = sample_probs.mean(dim=0)\n",
    "    \n",
    "    all_sample_preds[im_ind, :, :, :] = sample_probs.cpu().numpy()\n",
    "    predictions = probs.cpu().numpy()\n",
    "    all_preds[im_ind, :, :] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotations = (np.linspace(0, 179, steps)).astype(int)\n",
    "\n",
    "correct_preds = np.zeros((N, steps))\n",
    "for i in range(N):\n",
    "    correct_preds[i,:] = all_preds[i,:,y_dev[i]]   \n",
    "\n",
    "np.save(results_dir+'/correct_preds.npy', correct_preds)\n",
    "np.save(results_dir+'/all_preds.npy', all_preds)\n",
    "np.save(results_dir+'/all_sample_preds.npy', all_sample_preds)\n",
    "\n",
    "plot_predictive_entropy(correct_preds, all_preds, rotations, results_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10-C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chalPath = 'data/CIFAR-10-C/'\n",
    "chals = sorted(os.listdir(chalPath))\n",
    "\n",
    "chal_labels = valset.test_labels\n",
    "chal_labels = torch.Tensor(chal_labels)\n",
    "chal_labels = chal_labels.long()\n",
    "\n",
    "def preprocess_test(X):\n",
    "\n",
    "    N, H, W, C = X.shape\n",
    "    Y = torch.zeros(N, C, H, W)\n",
    "    mean = (0.4914, 0.4822, 0.4465)\n",
    "    std = (0.2023, 0.1994, 0.2010)  \n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean,std),\n",
    "    ])\n",
    "\n",
    "    for n in range(len(X)):\n",
    "        Y[n] =  transform_test(X[n])\n",
    "\n",
    "    return Y\n",
    "\n",
    "preds_list= []\n",
    "net.eval()\n",
    "avg_list = []\n",
    "\n",
    "for challenge in range(len(chals)):\n",
    "    chal_data = np.load(chalPath + chals[challenge])\n",
    "    # chal_data = np.transpose(chal_data, (0,3,1,2))\n",
    "\n",
    "    avg = 0\n",
    "    for j in range(5):\n",
    "        chal_temp_data = chal_data[j * 10000:(j + 1) * 10000]\n",
    "        chal_temp_data = preprocess_test(chal_temp_data)\n",
    "\n",
    "        chal_dataset = torch.utils.data.TensorDataset(chal_temp_data, chal_labels)\n",
    "        chal_loader = torch.utils.data.DataLoader(chal_dataset, batch_size=100)\n",
    "        chal_error = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x, y in chal_loader:\n",
    "                cost, err, probs = net.sample_eval(x, y, Nsamples=10, logits=False)\n",
    "                preds_list.append(probs)\n",
    "                chal_error += err.cpu().numpy()\n",
    "                # print(err)\n",
    "\n",
    "        # print(chal_error)\n",
    "        chal_acc = 1 - (chal_error/len(chal_dataset))\n",
    "        avg += chal_acc\n",
    "        print(chal_acc)\n",
    "    \n",
    "    avg /= 5\n",
    "    avg_list.append(avg)\n",
    "    print(\"Average:\",avg,\" \", chals[challenge])\n",
    "\n",
    "print(\"Mean: \", np.mean(avg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_list = np.vstack(preds_list)\n",
    "np.save(results_dir+'/preds_CIFAR-10-C.npy', preds_list)\n",
    "np.save(results_dir+'/avg_list_CIFAR-10-C.npy', avg_list)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eed44ad03d3398d64d3d4bda079817b6c24e225a548088d53b43c511791e0bd9"
  },
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit ('megabayes': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
