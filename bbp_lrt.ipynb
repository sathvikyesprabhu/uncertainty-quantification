{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "from networks import BBP_LRT\n",
    "import metrics\n",
    "from utils import *\n",
    "from utils_plotting import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "mean = (0.4914, 0.4822, 0.4465)\n",
    "std = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "transform_train = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.RandomCrop(32, padding=4),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean, std)\n",
    "                                    ])\n",
    "transform_test = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean, std)])\n",
    "\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "valset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "models_dir = \"Models/\" + 'BBP_models/' + 'lrt'\n",
    "results_dir = \"Results/\" + 'BBP_results/' + 'lrt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate network\n",
    "net = BBP_LRT(18, 10).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(net, criterion,trainloader, num_ens=1, beta_type=0.1, epoch=None, num_epochs=None):\n",
    "    net.train()\n",
    "    training_loss = 0.0\n",
    "    accs = []\n",
    "    kl_list = []\n",
    "    \n",
    "    lr = 0.01\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate(lr,epoch))\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(trainloader, 1):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = torch.zeros(inputs.shape[0], net.num_classes, num_ens).to(device)\n",
    "\n",
    "        kl = 0.0\n",
    "        for j in range(num_ens):\n",
    "            net_out, _kl = net(inputs)\n",
    "            kl += _kl\n",
    "            outputs[:, :, j] = F.log_softmax(net_out, dim=1)\n",
    "        \n",
    "        kl = kl / num_ens\n",
    "        kl_list.append(kl.item())\n",
    "        log_outputs = logmeanexp(outputs, dim=2)\n",
    "\n",
    "        beta = metrics.get_beta(i-1, len(trainloader), beta_type, epoch, num_epochs)\n",
    "        loss = criterion(log_outputs, labels, kl, beta)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        accs.append(metrics.acc(log_outputs.data, labels))\n",
    "        training_loss += loss.cpu().data.numpy()\n",
    "    return training_loss/len(trainloader), np.mean(accs), np.mean(kl_list)\n",
    "\n",
    "def validate_model(net, criterion, validloader, num_ens=1, beta_type=0.1, epoch=None, num_epochs=None):\n",
    "    \"\"\"Calculate ensemble accuracy and NLL Loss\"\"\"\n",
    "    net.eval()\n",
    "    valid_loss = 0.0\n",
    "    accs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(validloader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = torch.zeros(inputs.shape[0], net.num_classes, num_ens).to(device)\n",
    "            kl = 0.0\n",
    "            for j in range(num_ens):\n",
    "                net_out, _kl = net(inputs)\n",
    "                kl += _kl\n",
    "                outputs[:, :, j] = F.log_softmax(net_out, dim=1).data\n",
    "\n",
    "            log_outputs = logmeanexp(outputs, dim=2)\n",
    "\n",
    "            beta = metrics.get_beta(i-1, len(validloader), beta_type, epoch, num_epochs)\n",
    "            valid_loss += criterion(log_outputs, labels, kl, beta).item()\n",
    "            accs.append(metrics.acc(log_outputs, labels))\n",
    "\n",
    "    return valid_loss/len(validloader), np.mean(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "criterion = metrics.ELBO(len(trainset)).to(device)\n",
    "best_acc = 0.6\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_loss, train_acc, train_kl = train_model(net, criterion, trainloader, num_ens=1, beta_type=0.1, epoch=epoch, num_epochs=num_epochs)\n",
    "    valid_loss, acc = validate_model(net, criterion, valloader, num_ens=1, beta_type=0.1, epoch=epoch, num_epochs=num_epochs)\n",
    "\n",
    "    print('Epoch: {} | Train Loss: {:.4f} | Train Accuracy: {:.4f} | Val Loss: {:.4f} | Val Accuracy: {:.4f} | train_kl_div: {:.4f}'.format(\n",
    "            epoch, train_loss, train_acc, valid_loss, acc, train_kl))\n",
    "    \n",
    "    if acc > best_acc:\n",
    "        print('| Saving Best model...')\n",
    "        # state = {\n",
    "        #         'net':net.module if use_cuda else net,\n",
    "        #         'acc':acc,\n",
    "        #         'epoch':epoch,\n",
    "        # }\n",
    "\n",
    "        torch.save(net.state_dict(), models_dir + '/' + 'theta_best.t7')\n",
    "        best_acc = acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = BBP_LRT(18, 10).cuda()\n",
    "net.load_state_dict(torch.load(models_dir + '/' + 'theta_best.t7'))\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10 Rotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dev = []\n",
    "y_dev = []\n",
    "for x, y in valloader:\n",
    "    x_dev.append(x.cpu().numpy())\n",
    "    y_dev.append(y.cpu().numpy())\n",
    "\n",
    "x_dev = np.concatenate(x_dev)\n",
    "y_dev = np.concatenate(y_dev)\n",
    "print(x_dev.shape)\n",
    "print(y_dev.shape)\n",
    "\n",
    "plot_rotate(x_dev, y_dev, net, results_dir, im_list = valset.test_data,im_ind = 23, Nsamples = 100, steps=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rotated = np.load(\"data/CIFAR10_rotated.npy\")\n",
    "# data_rotated = np.transpose(data_rotated, (0,1,4,2,3))\n",
    "data_rotated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 16\n",
    "N = 10000\n",
    "Nsamples = 100\n",
    "y_dev = valset.test_labels\n",
    "\n",
    "def preprocess_test(X):\n",
    "\n",
    "    N, H, W, C = X.shape\n",
    "    Y = torch.zeros(N, C, H, W)\n",
    "    mean = (0.4914, 0.4822, 0.4465)\n",
    "    std = (0.2023, 0.1994, 0.2010)  \n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean,std),\n",
    "    ])\n",
    "\n",
    "    for n in range(len(X)):\n",
    "        Y[n] =  transform_test(X[n])\n",
    "\n",
    "    return Y\n",
    "\n",
    "all_preds = np.zeros((N, steps, 10))\n",
    "all_sample_preds = np.zeros((N, Nsamples, steps, 10))\n",
    "\n",
    "for im_ind in range(N):\n",
    "    if(im_ind % 500 == 0):\n",
    "        print(im_ind)\n",
    "\n",
    "    y =  y_dev[im_ind]\n",
    "    \n",
    "    ims = data_rotated[:,im_ind,:,:,:]\n",
    "    ims = preprocess_test(ims)\n",
    "\n",
    "    y = np.ones(ims.shape[0])*y\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        sample_probs = net.all_sample_eval(ims, torch.from_numpy(y), Nsamples=Nsamples)\n",
    "    probs = sample_probs.mean(dim=0)\n",
    "    \n",
    "    all_sample_preds[im_ind, :, :, :] = sample_probs.cpu().numpy()\n",
    "    predictions = probs.cpu().numpy()\n",
    "    all_preds[im_ind, :, :] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotations = (np.linspace(0, 179, steps)).astype(int)\n",
    "\n",
    "correct_preds = np.zeros((N, steps))\n",
    "for i in range(N):\n",
    "    correct_preds[i,:] = all_preds[i,:,y_dev[i]]   \n",
    "\n",
    "np.save(results_dir+'/correct_preds.npy', correct_preds)\n",
    "np.save(results_dir+'/all_preds.npy', all_preds)\n",
    "np.save(results_dir+'/all_sample_preds.npy', all_sample_preds)\n",
    "\n",
    "plot_predictive_entropy(correct_preds, all_preds, rotations, results_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10-C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chalPath = 'data/CIFAR-10-C/'\n",
    "chals = sorted(os.listdir(chalPath))\n",
    "\n",
    "chal_labels = valset.targets\n",
    "chal_labels = torch.Tensor(chal_labels)\n",
    "chal_labels = chal_labels.long()\n",
    "\n",
    "def preprocess_test(X):\n",
    "\n",
    "    N, H, W, C = X.shape\n",
    "    Y = torch.zeros(N, C, H, W)\n",
    "    mean = (0.4914, 0.4822, 0.4465)\n",
    "    std = (0.2023, 0.1994, 0.2010)  \n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean,std),\n",
    "    ])\n",
    "\n",
    "    for n in range(len(X)):\n",
    "        Y[n] =  transform_test(X[n])\n",
    "\n",
    "    return Y\n",
    "\n",
    "preds_list= []\n",
    "net.model.eval()\n",
    "avg_list = []\n",
    "\n",
    "for challenge in range(len(chals)):\n",
    "    chal_data = np.load(chalPath + chals[challenge])\n",
    "    # chal_data = np.transpose(chal_data, (0,3,1,2))\n",
    "\n",
    "    avg = 0\n",
    "    for j in range(5):\n",
    "        chal_temp_data = chal_data[j * 10000:(j + 1) * 10000]\n",
    "        chal_temp_data = preprocess_test(chal_temp_data)\n",
    "\n",
    "        chal_dataset = torch.utils.data.TensorDataset(chal_temp_data, chal_labels)\n",
    "        chal_loader = torch.utils.data.DataLoader(chal_dataset, batch_size=100)\n",
    "        chal_error = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x, y in chal_loader:\n",
    "                cost, err, probs = net.sample_eval(x, y, Nsamples=10, logits=False)\n",
    "                preds_list.append(probs.cpu().numpy())\n",
    "                chal_error += err.cpu().numpy()\n",
    "                # print(err)\n",
    "\n",
    "        # print(chal_error)\n",
    "        chal_acc = 1 - (chal_error/len(chal_dataset))\n",
    "        avg += chal_acc\n",
    "        print(chal_acc)\n",
    "    \n",
    "    avg /= 5\n",
    "    avg_list.append(avg)\n",
    "    print(\"Average:\",avg,\" \", chals[challenge])\n",
    "\n",
    "print(\"Mean: \", np.mean(avg_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_list = np.vstack(preds_list)\n",
    "np.save(results_dir+'/preds_CIFAR-10-C.npy', preds_list)\n",
    "np.save(results_dir+'/avg_list_CIFAR-10-C.npy', avg_list)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31b8918c388efabf28e06d7aa829f320b99f50374aee3ccc5c6fe8cd01c3acd4"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
